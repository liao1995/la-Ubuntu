WEBVTT

1
00:00:00.400 --> 00:00:01.739
在本段视频中 我想告诉大家

2
00:00:01.739 --> 00:00:05.133
一些关于梯度下降算法的实用技巧

3
00:00:05.133 --> 00:00:06.563
我将集中讨论

4
00:00:06.563 --> 00:00:09.414
学习率 α

5
00:00:09.551 --> 00:00:11.625
具体来说 这是梯度下降算法的

6
00:00:11.640 --> 00:00:13.677
更新规则

7
00:00:13.677 --> 00:00:14.908
这里我想要

8
00:00:14.908 --> 00:00:16.784
告诉大家

9
00:00:16.784 --> 00:00:18.629
如何调试

10
00:00:18.629 --> 00:00:19.994
也就是我认为应该如何确定

11
00:00:19.994 --> 00:00:22.385
梯度下降是正常工作的

12
00:00:22.390 --> 00:00:23.632
此外我还想告诉大家

13
00:00:23.632 --> 00:00:25.879
如何选择学习率 α

14
00:00:25.890 --> 00:00:27.079
也就是我平常

15
00:00:27.079 --> 00:00:29.222
如何选择这个参数

16
00:00:29.222 --> 00:00:30.702
我通常是怎样确定

17
00:00:30.702 --> 00:00:34.125
梯度下降正常工作的

18
00:00:34.125 --> 00:00:35.852
梯度下降算法所做的事情

19
00:00:35.852 --> 00:00:37.107
就是为你找到

20
00:00:37.107 --> 00:00:38.710
一个 θ 值

21
00:00:38.710 --> 00:00:42.692
并希望它能够最小化代价函数 J(θ)

22
00:00:42.692 --> 00:00:44.285
我通常会在

23
00:00:44.300 --> 00:00:46.121
梯度下降算法运行时

24
00:00:46.121 --> 00:00:49.731
绘出代价函数 J(θ) 的值

25
00:00:49.750 --> 00:00:51.367
这里的 x 轴是表示

26
00:00:51.367 --> 00:00:52.828
梯度下降算法的

27
00:00:52.850 --> 00:00:54.278
迭代步数

28
00:00:54.278 --> 00:00:55.985
你可能会得到

29
00:00:55.985 --> 00:00:59.722
这样一条曲线

30
00:00:59.722 --> 00:01:01.249
注意 这里的 x 轴

31
00:01:01.249 --> 00:01:03.592
是迭代步数

32
00:01:03.592 --> 00:01:05.098
在我们以前看到的

33
00:01:05.098 --> 00:01:07.050
J(θ) 曲线中

34
00:01:07.050 --> 00:01:08.931
x 轴 也就是横轴

35
00:01:08.950 --> 00:01:13.122
曾经用来表示参数 θ 但这里不是

36
00:01:13.122 --> 00:01:15.068
具体来说

37
00:01:15.090 --> 00:01:17.725
这一点的含义是这样的

38
00:01:17.725 --> 00:01:20.608
当我运行完100步的梯度下降迭代之后

39
00:01:20.608 --> 00:01:22.608
无论我得到

40
00:01:22.620 --> 00:01:24.095
什么 θ 值

41
00:01:24.110 --> 00:01:25.620
总之 100步迭代之后

42
00:01:25.620 --> 00:01:27.139
我将得到

43
00:01:27.150 --> 00:01:29.150
一个 θ 值

44
00:01:29.150 --> 00:01:30.683
根据100步迭代之后

45
00:01:30.683 --> 00:01:32.857
得到的这个 θ 值

46
00:01:32.857 --> 00:01:34.118
我将算出

47
00:01:34.120 --> 00:01:36.272
代价函数 J(θ) 的值

48
00:01:36.272 --> 00:01:37.718
而这个点的垂直高度就代表

49
00:01:37.718 --> 00:01:39.961
梯度下降算法

50
00:01:39.961 --> 00:01:41.135
100步迭代之后

51
00:01:41.135 --> 00:01:42.212
得到的 θ

52
00:01:42.220 --> 00:01:44.050
算出的 J(θ) 值

53
00:01:44.050 --> 00:01:46.515
而这个点

54
00:01:46.515 --> 00:01:48.253
则是梯度下降算法

55
00:01:48.253 --> 00:01:50.120
迭代200次之后

56
00:01:50.120 --> 00:01:52.097
得到的 θ

57
00:01:52.097 --> 00:01:55.172
算出的 J(θ) 值

58
00:01:55.172 --> 00:01:56.713
所以这条曲线

59
00:01:56.720 --> 00:01:58.177
显示的是

60
00:01:58.200 --> 00:02:02.025
梯度下降算法迭代过程中代价函数 J(θ) 的值

61
00:02:02.025 --> 00:02:03.335
如果梯度下降算法

62
00:02:03.350 --> 00:02:05.180
正常工作

63
00:02:05.190 --> 00:02:08.952
那么每一步迭代之后

64
00:02:08.952 --> 00:02:12.219
J(θ) 都应该下降

65
00:02:16.451 --> 00:02:19.248
这条曲线

66
00:02:19.248 --> 00:02:20.580
的一个用处在于

67
00:02:20.580 --> 00:02:22.545
它可以告诉你

68
00:02:22.545 --> 00:02:24.147
如果你看一下

69
00:02:24.160 --> 00:02:26.015
我画的这条曲线

70
00:02:26.030 --> 00:02:27.581
当你达到

71
00:02:27.581 --> 00:02:29.744
300步迭代之后

72
00:02:29.744 --> 00:02:31.348
也就是300步到400步迭代之间

73
00:02:31.348 --> 00:02:32.908
也就是曲线的这一段

74
00:02:32.910 --> 00:02:35.792
看起来 J(θ) 并没有下降多少

75
00:02:35.810 --> 00:02:36.930
所以当你

76
00:02:36.960 --> 00:02:38.785
到达400步迭代时

77
00:02:38.810 --> 00:02:41.554
这条曲线看起来已经很平坦了

78
00:02:41.554 --> 00:02:43.334
也就是说

79
00:02:43.340 --> 00:02:44.533
在这里400步迭代的时候

80
00:02:44.533 --> 00:02:45.847
梯度下降算法

81
00:02:45.850 --> 00:02:47.868
基本上已经收敛了

82
00:02:47.880 --> 00:02:50.493
因为代价函数并没有继续下降

83
00:02:50.493 --> 00:02:51.625
所以说 看这条曲线

84
00:02:51.625 --> 00:02:53.390
可以帮助你判断

85
00:02:53.420 --> 00:02:56.812
梯度下降算法是否已经收敛

86
00:02:57.550 --> 00:02:58.900
顺便说一下

87
00:02:58.900 --> 00:03:00.820
对于每一个特定的问题

88
00:03:00.820 --> 00:03:02.098
梯度下降算法所需的迭代次数

89
00:03:02.098 --> 00:03:04.251
可以相差很大

90
00:03:04.251 --> 00:03:06.092
也许对于某一个问题

91
00:03:06.130 --> 00:03:07.832
梯度下降算法

92
00:03:07.832 --> 00:03:10.198
只需要30步迭代就可以收敛

93
00:03:10.210 --> 00:03:12.670
然而换一个问题

94
00:03:12.670 --> 00:03:15.042
也许梯度下降算法就需要3000步迭代

95
00:03:15.050 --> 00:03:17.996
对于另一个机器学习问题

96
00:03:17.996 --> 00:03:19.823
则可能需要三百万步迭代

97
00:03:19.823 --> 00:03:20.768
实际上

98
00:03:20.768 --> 00:03:22.312
我们很难提前判断

99
00:03:22.312 --> 00:03:24.333
梯度下降算法

100
00:03:24.360 --> 00:03:26.252
需要多少步迭代才能收敛

101
00:03:26.252 --> 00:03:28.935
通常我们需要画出这类曲线

102
00:03:28.940 --> 00:03:32.958
画出代价函数随迭代步数数增加的变化曲线

103
00:03:32.960 --> 00:03:34.347
通常 我会通过看这种曲线

104
00:03:34.347 --> 00:03:35.610
来试着判断

105
00:03:35.610 --> 00:03:38.470
梯度下降算法是否已经收敛

106
00:03:38.590 --> 00:03:40.118
另外 也可以

107
00:03:40.130 --> 00:03:42.748
进行一些自动的收敛测试

108
00:03:42.748 --> 00:03:44.306
也就是说用一种算法

109
00:03:44.306 --> 00:03:46.593
来告诉你梯度下降算法

110
00:03:46.593 --> 00:03:48.615
是否已经收敛

111
00:03:48.620 --> 00:03:50.268
自动收敛测试

112
00:03:50.268 --> 00:03:52.505
一个非常典型的例子是

113
00:03:52.540 --> 00:03:54.981
如果代价函数 J(θ)

114
00:03:54.981 --> 00:03:57.009
的下降小于

115
00:03:57.020 --> 00:03:58.396
一个很小的值 ε

116
00:03:58.396 --> 00:04:01.435
那么就认为已经收敛

117
00:04:01.435 --> 00:04:02.412
比如可以选择

118
00:04:02.412 --> 00:04:05.272
1e-3

119
00:04:05.272 --> 00:04:07.065
但我发现

120
00:04:07.070 --> 00:04:10.740
通常要选择一个合适的阈值 ε 是相当困难的

121
00:04:10.740 --> 00:04:12.049
因此 为了检查

122
00:04:12.049 --> 00:04:14.058
梯度下降算法是否收敛

123
00:04:14.090 --> 00:04:15.361
我实际上还是

124
00:04:15.361 --> 00:04:17.074
通过看

125
00:04:17.074 --> 00:04:18.299
左边的这条曲线图

126
00:04:18.310 --> 00:04:21.778
而不是依靠自动收敛测试

127
00:04:21.778 --> 00:04:22.778
此外 这种曲线图

128
00:04:22.780 --> 00:04:24.340
也可以

129
00:04:24.340 --> 00:04:25.812
在算法没有正常工作时

130
00:04:25.820 --> 00:04:28.659
提前警告你

131
00:04:28.690 --> 00:04:30.185
具体地说

132
00:04:30.200 --> 00:04:31.641
如果代价函数 J(θ)

133
00:04:31.650 --> 00:04:34.850
随迭代步数

134
00:04:34.850 --> 00:04:35.864
的变化曲线是这个样子

135
00:04:35.864 --> 00:04:37.105
J(θ) 实际上在不断上升

136
00:04:37.120 --> 00:04:39.136
那么这就很明确的表示

137
00:04:39.136 --> 00:04:42.898
梯度下降算法没有正常工作

138
00:04:42.898 --> 00:04:44.552
而这样的曲线图

139
00:04:44.552 --> 00:04:48.253
通常意味着你应该使用较小的学习率 α

140
00:04:48.270 --> 00:04:49.697
如果 J(θ) 在上升

141
00:04:49.697 --> 00:04:51.564
那么最常见的原因是

142
00:04:51.580 --> 00:04:53.199
你在最小化

143
00:04:53.199 --> 00:04:54.904
这样的

144
00:04:54.904 --> 00:04:59.346
一个函数

145
00:04:59.346 --> 00:05:00.518
这时如果你的学习率太大

146
00:05:00.518 --> 00:05:01.614
当你从这里开始

147
00:05:01.614 --> 00:05:03.202
梯度下降算法

148
00:05:03.202 --> 00:05:05.516
可能将冲过最小值达到这里

149
00:05:05.516 --> 00:05:07.145
而如果你的学习率太大

150
00:05:07.145 --> 00:05:08.473
你可能再次冲过最小值

151
00:05:08.500 --> 00:05:10.493
达到这里

152
00:05:10.500 --> 00:05:12.279
然后一直这样下去

153
00:05:12.279 --> 00:05:13.810
而你真正想要的是

154
00:05:13.810 --> 00:05:17.991
从这里开始慢慢的下降

155
00:05:17.991 --> 00:05:19.465
但是 如果学习率过大

156
00:05:19.465 --> 00:05:21.252
那么梯度下降算法

157
00:05:21.252 --> 00:05:22.749
将会不断的

158
00:05:22.760 --> 00:05:24.454
冲过最小值

159
00:05:24.454 --> 00:05:26.147
然后你将得到

160
00:05:26.160 --> 00:05:27.170
越来越糟糕的结果

161
00:05:27.210 --> 00:05:28.720
得到越来越大的

162
00:05:28.780 --> 00:05:30.744
代价函数 J(θ) 值

163
00:05:30.744 --> 00:05:31.751
所以如果你得到了

164
00:05:31.751 --> 00:05:33.263
这样一个曲线图

165
00:05:33.263 --> 00:05:34.248
如果你看到这样一个曲线图

166
00:05:34.248 --> 00:05:36.106
通常的解决方法是

167
00:05:36.106 --> 00:05:38.182
使用较小的 α 值

168
00:05:38.182 --> 00:05:39.640
当然也要确保

169
00:05:39.790 --> 00:05:41.872
你的代码中没有错误

170
00:05:41.872 --> 00:05:43.268
但通常最可能

171
00:05:43.268 --> 00:05:44.709
出现的错误是

172
00:05:44.709 --> 00:05:48.105
α 值过大

173
00:05:49.050 --> 00:05:50.595
同样的 有时你可能

174
00:05:50.595 --> 00:05:52.115
看到这种形状的

175
00:05:52.120 --> 00:05:53.188
J(θ) 曲线

176
00:05:53.188 --> 00:05:54.158
它先下降 然后上升

177
00:05:54.160 --> 00:05:56.325
接着又下降 然后又上升

178
00:05:56.330 --> 00:05:57.366
然后再次下降

179
00:05:57.366 --> 00:05:58.910
再次上升 如此往复

180
00:05:58.930 --> 00:06:00.150
而解决这种情况的方法

181
00:06:00.150 --> 00:06:04.052
通常同样是选择较小 α 值

182
00:06:04.090 --> 00:06:05.129
我不打算证明这一点

183
00:06:05.129 --> 00:06:07.128
但对于我们讨论的线性回归

184
00:06:07.128 --> 00:06:10.806
可以很容易从数学上证明

185
00:06:10.830 --> 00:06:12.608
只要学习率足够小

186
00:06:12.608 --> 00:06:13.913
那么每次迭代之后

187
00:06:13.913 --> 00:06:15.885
代价函数 J(θ)

188
00:06:15.885 --> 00:06:19.025
都会下降

189
00:06:19.030 --> 00:06:21.342
因此如果代价函数没有下降

190
00:06:21.342 --> 00:06:22.338
那可能以为着学习率过大

191
00:06:22.338 --> 00:06:23.992
这时你就应该尝试一个较小的学习率

192
00:06:23.992 --> 00:06:24.867
当然 你也不希望

193
00:06:24.890 --> 00:06:25.788
学习度太小

194
00:06:25.788 --> 00:06:27.068
因为如果这样

195
00:06:27.070 --> 00:06:28.095
如果你这么做

196
00:06:28.095 --> 00:06:31.543
那么梯度下降算法可能收敛得很慢

197
00:06:31.543 --> 00:06:32.812
如果学习率 α 太小

198
00:06:32.812 --> 00:06:34.804
你可能

199
00:06:34.804 --> 00:06:36.945
从这里开始

200
00:06:36.960 --> 00:06:38.248
然后很缓慢很缓慢

201
00:06:38.248 --> 00:06:40.408
向最低点移动

202
00:06:40.408 --> 00:06:41.338
这样一来

203
00:06:41.338 --> 00:06:42.974
你需要迭代很多次

204
00:06:42.980 --> 00:06:47.064
才能到达最低点

205
00:06:47.090 --> 00:06:48.118
因此 如果学习率 α 太小

206
00:06:48.118 --> 00:06:49.500
梯度下降算法

207
00:06:49.570 --> 00:06:52.989
的收敛将会很缓慢

208
00:06:53.005 --> 00:06:55.377
总结一下

209
00:06:55.377 --> 00:06:57.301
如果学习率 α 太小

210
00:06:57.301 --> 00:06:59.672
你会遇到收敛速度慢的问题

211
00:06:59.672 --> 00:07:01.161
而如果学习率 α 太大

212
00:07:01.161 --> 00:07:02.494
代价函数 J(θ) 可能不会在

213
00:07:02.494 --> 00:07:04.378
每次迭代都下降

214
00:07:04.378 --> 00:07:06.023
甚至可能不收敛

215
00:07:06.023 --> 00:07:08.579
在某些情况下

216
00:07:08.579 --> 00:07:10.957
如果学习率 α 过大

217
00:07:10.990 --> 00:07:14.710
也可能出现收敛缓慢的问题

218
00:07:14.800 --> 00:07:16.312
但更常见的情况是

219
00:07:16.312 --> 00:07:17.380
你会发现代价函数 J(θ)

220
00:07:17.440 --> 00:07:20.532
并不会在每次迭代之后都下降

221
00:07:20.540 --> 00:07:22.223
而为了调试

222
00:07:22.223 --> 00:07:24.539
所有这些情况

223
00:07:24.539 --> 00:07:26.053
绘制J(θ)随迭代步数变化的曲线

224
00:07:26.070 --> 00:07:29.315
通常可以帮助你弄清楚到底发生了什么

225
00:07:29.315 --> 00:07:31.258
具体来说

226
00:07:31.258 --> 00:07:32.525
当我运行梯度下降算法时

227
00:07:32.525 --> 00:07:34.997
我通常会尝试一系列α值

228
00:07:35.000 --> 00:07:36.555
所以在运行梯度下降算法制

229
00:07:36.580 --> 00:07:37.988
请尝试不同的 α 值

230
00:07:37.988 --> 00:07:39.902
比如0.001, 0.01

231
00:07:39.902 --> 00:07:41.471
这里每隔10倍

232
00:07:41.471 --> 00:07:43.275
取一个值

233
00:07:43.280 --> 00:07:44.449
然后对于这些不同的 α 值

234
00:07:44.449 --> 00:07:45.769
绘制 J(θ)

235
00:07:45.769 --> 00:07:47.015
随迭代步数变化的曲线

236
00:07:47.030 --> 00:07:49.202
然后选择

237
00:07:49.202 --> 00:07:51.094
看上去使得 J(θ)

238
00:07:51.094 --> 00:07:54.805
快速下降的一个 α 值

239
00:07:54.805 --> 00:07:58.067
事实上 我通常并不是隔10倍取一个值

240
00:07:58.067 --> 00:07:59.305
你可以看到

241
00:07:59.305 --> 00:08:01.780
这里是每隔10倍取一个值

242
00:08:01.869 --> 00:08:03.860
我通常取的

243
00:08:03.870 --> 00:08:07.164
是这些 α 值

244
00:08:07.164 --> 00:08:09.770
一直这样下去

245
00:08:09.816 --> 00:08:11.365
你看 先取0.001

246
00:08:11.365 --> 00:08:13.598
然后将学习率增加3倍

247
00:08:13.598 --> 00:08:15.182
得到0.003

248
00:08:15.182 --> 00:08:17.356
然后这一步

249
00:08:17.356 --> 00:08:20.187
从0.003到0.01

250
00:08:20.187 --> 00:08:22.145
又大约增加了3倍

251
00:08:22.145 --> 00:08:24.605
所以 在为梯度下降算法

252
00:08:24.605 --> 00:08:27.343
选择合适的学习率时

253
00:08:27.343 --> 00:08:28.580
我大致是

254
00:08:28.580 --> 00:08:30.592
按3的倍数来取值的

255
00:08:30.592 --> 00:08:32.256
所以我会尝试一系列α值

256
00:08:32.256 --> 00:08:33.495
直到我找到

257
00:08:33.495 --> 00:08:34.725
一个值

258
00:08:34.725 --> 00:08:35.757
它不能再小了

259
00:08:35.757 --> 00:08:37.137
同时找到另一个值

260
00:08:37.137 --> 00:08:38.394
它不能再大了

261
00:08:38.394 --> 00:08:40.560
然后我尽量挑选

262
00:08:40.560 --> 00:08:42.279
其中最大的那个 α 值

263
00:08:42.279 --> 00:08:43.764
或者一个比最大值

264
00:08:43.764 --> 00:08:46.488
略小一些的合理的值

265
00:08:46.488 --> 00:08:48.178
而当我做了以上工作时

266
00:08:48.178 --> 00:08:49.592
我通常就可以得到

267
00:08:49.592 --> 00:08:51.968
一个不错的学习率

268
00:08:51.968 --> 00:08:53.203
如果也你这样做

269
00:08:53.203 --> 00:08:54.345
那么你也能够

270
00:08:54.345 --> 00:08:55.906
为你的梯度下降算法

271
00:08:55.906 --> 00:08:57.340
找到一个合适的

272
00:08:57.340 --> 00:08:58.563
学习率值 【教育无边界字幕组】翻译：王祖超 校对：所罗门捷列夫 审核：Naplessss