WEBVTT

1
00:00:00.290 --> 00:00:01.510
在之前的视频中 我们讨论了

2
00:00:01.840 --> 00:00:02.770
如何使用前向传播

3
00:00:03.570 --> 00:00:05.200
和反向传播

4
00:00:05.250 --> 00:00:07.560
计算神经网络中的导数

5
00:00:08.800 --> 00:00:10.070
但反向传播作为一个

6
00:00:10.580 --> 00:00:11.910
有很多细节的算法

7
00:00:12.170 --> 00:00:12.920
在实现的时候

8
00:00:13.050 --> 00:00:14.930
会有点复杂

9
00:00:15.700 --> 00:00:17.480
而且有一个不好的方面是

10
00:00:17.750 --> 00:00:18.690
在实现反向传播时

11
00:00:18.780 --> 00:00:20.080
会遇到很多细小的错误

12
00:00:20.320 --> 00:00:22.000
所以如果你把它和梯度下降法

13
00:00:22.140 --> 00:00:23.130
或者其他优化算法一起运行时

14
00:00:23.480 --> 00:00:26.590
可能看起来它运行正常

15
00:00:27.240 --> 00:00:28.480
并且 你的代价函数J

16
00:00:28.700 --> 00:00:29.930
最后可能

17
00:00:30.090 --> 00:00:31.240
在每次梯度下降法迭代时

18
00:00:31.830 --> 00:00:33.660
都会减小

19
00:00:33.830 --> 00:00:35.180
即使在实现反向传播时有一些小错误

20
00:00:35.440 --> 00:00:37.690
可能也会检查不出来

21
00:00:38.400 --> 00:00:39.280
所以它看起来是

22
00:00:39.360 --> 00:00:40.830
J(θ)在减小

23
00:00:40.920 --> 00:00:42.230
但是可能你最后得到的神经网络

24
00:00:42.410 --> 00:00:43.760
但是可能你最后得到的

25
00:00:43.880 --> 00:00:44.970
神经网络误差

26
00:00:45.490 --> 00:00:46.540
比没有错误的要高

27
00:00:46.780 --> 00:00:48.130
而且你很可能

28
00:00:48.330 --> 00:00:49.330
就是不知道

29
00:00:49.460 --> 00:00:50.470
你的结果是这些

30
00:00:50.530 --> 00:00:52.260
小错误导致的

31
00:00:52.950 --> 00:00:53.320
那你应该怎么办呢

32
00:00:54.160 --> 00:00:55.940
有一个想法叫梯度检验 (Gradient Checking)

33
00:00:56.790 --> 00:00:58.720
可以解决基本所有的问题

34
00:00:59.250 --> 00:01:00.550
我现在每次实现

35
00:01:00.770 --> 00:01:02.150
神经网络的反向传播

36
00:01:02.370 --> 00:01:03.320
或者类似的

37
00:01:03.450 --> 00:01:04.950
梯度下降算法

38
00:01:05.640 --> 00:01:07.310
或者其他比较复杂的模型

39
00:01:07.540 --> 00:01:08.840
我都会使用梯度检验

40
00:01:09.650 --> 00:01:10.610
如果你这么做

41
00:01:10.730 --> 00:01:12.010
它会帮你确定

42
00:01:12.140 --> 00:01:13.410
并且能很确信

43
00:01:13.540 --> 00:01:14.940
你实现的前向传播和反向传播

44
00:01:15.370 --> 00:01:17.430
或者其他的什么 是100%正确的

45
00:01:18.240 --> 00:01:19.090
我见过很多

46
00:01:19.330 --> 00:01:20.880
这样解决那些

47
00:01:21.160 --> 00:01:23.090
实现时容易有

48
00:01:23.420 --> 00:01:25.790
有小错误的问题

49
00:01:26.330 --> 00:01:27.470
在之前的视频中

50
00:01:28.170 --> 00:01:29.120
我一般是让你相信

51
00:01:29.390 --> 00:01:30.950
我给出的计算

52
00:01:31.170 --> 00:01:33.000
δ，d项

53
00:01:33.110 --> 00:01:34.220
等等之类的公式

54
00:01:34.260 --> 00:01:35.480
我要求你们相信

55
00:01:36.330 --> 00:01:37.600
他们计算的就是

56
00:01:38.180 --> 00:01:39.790
代价函数的梯度

57
00:01:40.150 --> 00:01:41.740
但一旦你们实现数值梯度检验

58
00:01:42.130 --> 00:01:43.210
也就是这节视频的主题

59
00:01:43.800 --> 00:01:45.250
你就能够自己验证

60
00:01:45.350 --> 00:01:46.490
你写的代码

61
00:01:46.610 --> 00:01:48.530
确实是在计算

62
00:01:49.600 --> 00:01:50.520
代价函数J的导数

63
00:01:50.820 --> 00:01:53.060
想法是这样的

64
00:01:53.550 --> 00:01:54.520
考虑下面这个例子

65
00:01:55.450 --> 00:01:56.230
假如我有一个

66
00:01:56.710 --> 00:01:58.140
函数J(θ)

67
00:01:58.250 --> 00:02:01.320
并且我有个值 θ

68
00:02:01.610 --> 00:02:04.380
在这个例子中 我假定θ只是一个实数

69
00:02:05.470 --> 00:02:08.210
假如说我想估计这个函数在这一点的导数

70
00:02:08.710 --> 00:02:10.220
这个导数等于

71
00:02:10.750 --> 00:02:13.190
这条切线的斜率

72
00:02:14.270 --> 00:02:15.420
下面我要用数值方法

73
00:02:16.180 --> 00:02:17.840
来计算近似的导数

74
00:02:17.970 --> 00:02:19.190
这个是用数值方法

75
00:02:19.780 --> 00:02:21.480
计算近似导数的过程

76
00:02:21.800 --> 00:02:23.520
我要计算θ+ε

77
00:02:24.000 --> 00:02:25.550
这个值在右边一点

78
00:02:26.340 --> 00:02:27.900
然后计算θ-ε

79
00:02:28.410 --> 00:02:30.800
然后看这两个点

80
00:02:30.950 --> 00:02:34.360
用一条直线

81
00:02:34.840 --> 00:02:35.860
把它们连起来

82
00:02:43.160 --> 00:02:44.280
我要把这两个点

83
00:02:44.480 --> 00:02:45.490
用一条直线连起来

84
00:02:45.680 --> 00:02:46.430
然后用这条

85
00:02:46.480 --> 00:02:47.740
红色线的斜率

86
00:02:48.000 --> 00:02:49.200
来作为我

87
00:02:49.390 --> 00:02:50.940
导数的近似值

88
00:02:51.460 --> 00:02:53.110
真正的导数是这边这条

89
00:02:53.280 --> 00:02:54.740
蓝色线的斜率

90
00:02:55.260 --> 00:02:56.660
这看起来是个不错的近似

91
00:02:58.220 --> 00:02:59.450
在数学上 这条红线的斜率等于

92
00:02:59.670 --> 00:03:01.340
这个垂直的高度

93
00:03:01.890 --> 00:03:03.680
除以这个

94
00:03:03.890 --> 00:03:05.580
这个水平的宽度

95
00:03:05.840 --> 00:03:07.500
所以上面这点

96
00:03:08.920 --> 00:03:10.840
是J(θ+ε)

97
00:03:11.140 --> 00:03:13.020
这点是J(Θ-ε)

98
00:03:13.830 --> 00:03:15.450
垂直方向上的

99
00:03:15.670 --> 00:03:17.530
差是J(θ+ε)-J(θ+ε)

100
00:03:17.810 --> 00:03:18.810
也就是说

101
00:03:19.700 --> 00:03:21.730
水平的距离就是2ε

102
00:03:23.620 --> 00:03:25.340
那么 我的近似是这样的

103
00:03:25.410 --> 00:03:27.280
J(θ)

104
00:03:29.110 --> 00:03:30.160
对θ的导数

105
00:03:30.490 --> 00:03:32.170
近似等于

106
00:03:32.320 --> 00:03:34.950
J(θ+ε)-J(θ-ε)

107
00:03:35.150 --> 00:03:36.860
除以2ε

108
00:03:37.460 --> 00:03:40.600
近似于J(θ+ε)-J(θ-ε) 除以2ε

109
00:03:42.280 --> 00:03:43.330
通常

110
00:03:43.600 --> 00:03:44.790
我给ε取很小的值

111
00:03:45.040 --> 00:03:46.270
比如可能取

112
00:03:46.530 --> 00:03:48.220
10的-4次方

113
00:03:48.740 --> 00:03:49.890
ε的取值在一个

114
00:03:50.190 --> 00:03:52.280
很大范围内都是可行的

115
00:03:53.050 --> 00:03:54.470
实际上

116
00:03:55.280 --> 00:03:56.540
如果你让ε非常小

117
00:03:57.010 --> 00:03:58.580
那么 数学上

118
00:03:59.210 --> 00:04:00.790
这里这项实际上就是导数

119
00:04:01.000 --> 00:04:02.340
就变成了函数

120
00:04:02.860 --> 00:04:04.310
在这点上准确的斜率

121
00:04:05.050 --> 00:04:05.730
只是我们不想用

122
00:04:05.910 --> 00:04:06.980
非常非常小的ε

123
00:04:07.170 --> 00:04:09.630
因为可能会产生数值问题

124
00:04:10.130 --> 00:04:11.070
所以我通常让ε

125
00:04:11.380 --> 00:04:14.200
差不多等于10^-4

126
00:04:14.470 --> 00:04:15.220
顺便说一下 可能你们有些学习者

127
00:04:15.330 --> 00:04:17.590
见过另外这种

128
00:04:17.750 --> 00:04:19.710
估计导数的公式

129
00:04:21.590 --> 00:04:23.500
右边这个叫做单侧拆分

130
00:04:24.040 --> 00:04:26.580
左边这个公式叫做双侧差分

131
00:04:27.120 --> 00:04:28.670
双侧差分给我们了一个

132
00:04:28.890 --> 00:04:29.750
稍微精确些的估计

133
00:04:30.170 --> 00:04:31.410
所以我通常用那个

134
00:04:31.670 --> 00:04:33.540
而不用这个单侧差分估计

135
00:04:35.900 --> 00:04:37.280
具体地说 你在Octave中实现时

136
00:04:37.750 --> 00:04:39.280
要使用下面这个

137
00:04:40.270 --> 00:04:41.490
你的程序要调用

138
00:04:41.600 --> 00:04:43.160
gradApprox来计算

139
00:04:43.270 --> 00:04:44.590
这个函数

140
00:04:45.380 --> 00:04:46.820
会通过这个公式

141
00:04:47.200 --> 00:04:48.550
J(θ+ε)-J(θ-ε)

142
00:04:48.730 --> 00:04:50.800
除以2ε

143
00:04:52.060 --> 00:04:52.980
它会给出这点导数的

144
00:04:53.100 --> 00:04:56.110
数值估计

145
00:04:56.590 --> 00:04:58.910
在这个例子中 它看起来是个很好的估计

146
00:05:01.970 --> 00:05:03.460
在之前的幻灯片中

147
00:05:03.710 --> 00:05:05.040
我们考虑了

148
00:05:05.290 --> 00:05:07.010
θ是一个实数的情况

149
00:05:08.000 --> 00:05:08.670
现在我们看更普遍的情况

150
00:05:08.900 --> 00:05:11.650
θ是一个向量参数

151
00:05:12.220 --> 00:05:13.270
假如说θ是n维向量

152
00:05:13.520 --> 00:05:14.610
它可能是我们的

153
00:05:15.000 --> 00:05:16.510
神经网络参数的

154
00:05:16.610 --> 00:05:18.010
展开形式

155
00:05:18.250 --> 00:05:19.580
所以θ是一个有

156
00:05:19.800 --> 00:05:21.230
有n个元素的向量

157
00:05:21.350 --> 00:05:25.100
θ1到θn

158
00:05:25.240 --> 00:05:26.530
我们可以用类似的想法

159
00:05:27.080 --> 00:05:29.300
来估计所有的偏导数项

160
00:05:30.250 --> 00:05:31.730
具体地说

161
00:05:32.420 --> 00:05:33.840
代价函数对

162
00:05:34.110 --> 00:05:35.710
第一个参数θ1取偏导数

163
00:05:36.110 --> 00:05:37.270
它可以用J

164
00:05:37.410 --> 00:05:40.270
和增大的θ1得到

165
00:05:40.380 --> 00:05:43.030
所以你有J(θ1+ε) 等等

166
00:05:43.520 --> 00:05:44.780
减去J(θ1-ε)

167
00:05:45.520 --> 00:05:46.820
然后除以2ε

168
00:05:48.130 --> 00:05:49.660
对第二个参数θ2

169
00:05:49.740 --> 00:05:51.090
取偏导数

170
00:05:51.620 --> 00:05:53.130
还是这样

171
00:05:53.270 --> 00:05:54.370
 除了你要对

172
00:05:54.740 --> 00:05:56.240
θ2+ε取J

173
00:05:56.570 --> 00:05:58.290
这里还有θ2-ε

174
00:05:59.100 --> 00:06:00.170
这样计算后面的偏导数

175
00:06:00.260 --> 00:06:01.680
直到θn

176
00:06:01.780 --> 00:06:02.780
它的算法是

177
00:06:03.030 --> 00:06:04.550
对θn增加

178
00:06:05.060 --> 00:06:06.140
和减少ε

179
00:06:09.790 --> 00:06:11.550
这些公式

180
00:06:11.720 --> 00:06:13.580
给出一个计算J

181
00:06:14.690 --> 00:06:16.500
对任意参数求偏导数的

182
00:06:17.250 --> 00:06:20.100
数值近似的方法

183
00:06:23.640 --> 00:06:26.030
具体地说 你要实现的是下面这个

184
00:06:27.900 --> 00:06:29.260
我们把这个用在Octave里

185
00:06:29.820 --> 00:06:31.000
来计算数值导数

186
00:06:32.220 --> 00:06:33.670
假如 i 

187
00:06:33.790 --> 00:06:35.110
等于 1 到 n

188
00:06:35.310 --> 00:06:37.140
n是我们的参数向量θ的维度

189
00:06:37.730 --> 00:06:40.680
我通常用参数的展开形式来计算

190
00:06:41.250 --> 00:06:42.210
你知道θ只是我们

191
00:06:42.530 --> 00:06:44.770
神经网络模型的一长列参数

192
00:06:46.230 --> 00:06:47.550
我让thetaPlus等于theta

193
00:06:47.830 --> 00:06:49.270
然后给thetaPlus的第 i 项

194
00:06:49.630 --> 00:06:51.170
加上EPSILON

195
00:06:51.660 --> 00:06:53.010
这就是基本的

196
00:06:53.720 --> 00:06:54.830
thetaPlus等于theta

197
00:06:55.340 --> 00:06:56.280
除了thetaPlus(i)

198
00:06:56.580 --> 00:06:57.820
它会增加EPSILON

199
00:06:58.310 --> 00:06:59.400
所以如果thetaPlus

200
00:07:00.810 --> 00:07:01.880
等于θ1 θ2 等等

201
00:07:01.970 --> 00:07:03.370
那么θi

202
00:07:04.020 --> 00:07:05.160
增加了EPSILON

203
00:07:05.350 --> 00:07:06.590
然后一直到θn

204
00:07:06.780 --> 00:07:08.440
这就是thetaPlus的作用

205
00:07:08.690 --> 00:07:11.340
类似的 这两行

206
00:07:11.530 --> 00:07:13.380
给thetaMinus

207
00:07:13.480 --> 00:07:15.090
类似地赋值

208
00:07:15.560 --> 00:07:16.720
只是θi不是加EPSILON

209
00:07:16.930 --> 00:07:19.150
而是减EPSILON

210
00:07:20.670 --> 00:07:22.320
最后 你运行这个

211
00:07:22.830 --> 00:07:24.370
gradApprox(i)

212
00:07:25.190 --> 00:07:26.430
它会给你近似的

213
00:07:27.210 --> 00:07:28.420
J(θ)对θi的

214
00:07:28.800 --> 00:07:30.250
偏导数

215
00:07:30.430 --> 00:07:32.430
我们实现

216
00:07:35.330 --> 00:07:36.420
神经网络时

217
00:07:36.760 --> 00:07:38.530
是这样用的

218
00:07:38.850 --> 00:07:41.530
我们要实现这个

219
00:07:41.770 --> 00:07:43.310
用for循环来计算

220
00:07:44.080 --> 00:07:45.570
代价函数对

221
00:07:45.860 --> 00:07:48.570
每个网络中的参数的偏导数

222
00:07:49.450 --> 00:07:51.120
然后我们用从

223
00:07:51.350 --> 00:07:53.070
反向传播得到的梯度

224
00:07:53.740 --> 00:07:55.110
DVec是我们从反向传播中

225
00:07:55.770 --> 00:07:57.150
得到的导数

226
00:07:58.380 --> 00:08:00.610
所以后向传播是一个

227
00:08:00.890 --> 00:08:02.030
相对比较有效率的

228
00:08:02.090 --> 00:08:03.350
计算代价函数

229
00:08:03.430 --> 00:08:04.970
对参数的导数

230
00:08:05.110 --> 00:08:06.850
或偏导数的方法

231
00:08:07.820 --> 00:08:08.960
接下来

232
00:08:09.350 --> 00:08:10.820
我通常做的是

233
00:08:11.440 --> 00:08:12.830
计算数值导数

234
00:08:12.960 --> 00:08:14.080
就是gradApprox

235
00:08:14.250 --> 00:08:15.830
我们刚从上面这里得到的

236
00:08:15.920 --> 00:08:17.030
来确定它等于

237
00:08:17.290 --> 00:08:19.420
或者近似于

238
00:08:19.980 --> 00:08:21.080
差距很小

239
00:08:21.810 --> 00:08:22.770
非常接近我们

240
00:08:22.970 --> 00:08:25.640
从反向传播得到的DVec

241
00:08:26.510 --> 00:08:27.460
如果这两种

242
00:08:27.930 --> 00:08:29.550
计算导数的方法

243
00:08:29.650 --> 00:08:31.040
给你相同的结果或者非常接近结果

244
00:08:31.300 --> 00:08:33.670
最多几位小数的差距

245
00:08:34.720 --> 00:08:36.560
那么我就非常确信

246
00:08:36.710 --> 00:08:38.720
我实现的反向传播时正确的

247
00:08:40.000 --> 00:08:41.230
然后我把这些DVec向量用在

248
00:08:41.660 --> 00:08:43.320
梯度下降法或者

249
00:08:43.760 --> 00:08:45.610
其他高级优化算法里

250
00:08:45.760 --> 00:08:46.850
然后我就可以比较确信

251
00:08:47.100 --> 00:08:48.870
我计算的导数

252
00:08:49.360 --> 00:08:51.010
是正确的

253
00:08:51.450 --> 00:08:52.670
那么 我的代码

254
00:08:52.790 --> 00:08:53.890
应该也可以正确运行

255
00:08:53.980 --> 00:08:55.570
可以很好地优化J(θ)

256
00:08:57.700 --> 00:08:58.680
最后 我想把

257
00:08:58.860 --> 00:09:00.050
所有的东西放在一起

258
00:09:00.310 --> 00:09:02.950
然后告诉你怎么实现这个数值梯度检验

259
00:09:03.630 --> 00:09:04.370
这是我通常做的

260
00:09:04.970 --> 00:09:06.020
第一件事

261
00:09:06.500 --> 00:09:08.180
是实现反向传播来计算DVec

262
00:09:08.490 --> 00:09:09.560
这个步骤是我们

263
00:09:09.830 --> 00:09:11.250
之前的视频中讲过的

264
00:09:11.490 --> 00:09:13.530
计算DVec 它可能是这些矩阵的展开形式

265
00:09:15.410 --> 00:09:16.550
然后我要做的是

266
00:09:17.010 --> 00:09:20.130
用gradApprox实现数值梯度检验

267
00:09:20.590 --> 00:09:23.550
这是我在这节视频前面部分讲的 在之前的幻灯片里

268
00:09:24.900 --> 00:09:27.680
然后你要确定DVec和gradApprox给出接近的结果

269
00:09:28.170 --> 00:09:30.860
可能最多差几位小数

270
00:09:32.270 --> 00:09:33.160
最后

271
00:09:33.240 --> 00:09:35.230
这是最重要的一步

272
00:09:35.480 --> 00:09:36.690
在使用你的代码去学习

273
00:09:37.000 --> 00:09:38.220
训练你的网络之前

274
00:09:38.570 --> 00:09:40.960
重要的是要关掉梯度检验

275
00:09:41.490 --> 00:09:42.800
不再使用

276
00:09:43.630 --> 00:09:44.940
这节视频前面讲的

277
00:09:45.250 --> 00:09:47.660
这个数值导数公式

278
00:09:47.980 --> 00:09:48.950
来计算

279
00:09:50.560 --> 00:09:50.560
gradApprox

280
00:09:50.960 --> 00:09:52.180
这样做的原因是

281
00:09:52.330 --> 00:09:53.800
我们之前讲的这个

282
00:09:54.120 --> 00:09:54.930
数值梯度检验代码

283
00:09:55.010 --> 00:09:56.220
是一个计算量

284
00:09:56.650 --> 00:09:58.570
非常大的程序

285
00:09:58.600 --> 00:10:00.960
它是一个非常慢的计算近似导数的方法

286
00:10:02.080 --> 00:10:03.490
而相对地

287
00:10:03.900 --> 00:10:04.710
我们之前讲的

288
00:10:04.940 --> 00:10:06.120
反向传播算法

289
00:10:06.370 --> 00:10:07.270
也就是那个

290
00:10:07.460 --> 00:10:08.900
DVec的D(1) D(2) D(3)的算法

291
00:10:09.320 --> 00:10:11.620
反向传播是一个在计算导数上

292
00:10:11.790 --> 00:10:14.930
效率更高的方法

293
00:10:17.070 --> 00:10:18.650
所以当你确认了

294
00:10:18.770 --> 00:10:20.270
你的反向传播算法是正确的

295
00:10:20.620 --> 00:10:21.840
你应该关掉梯度检验

296
00:10:22.160 --> 00:10:24.140
就是不使用它

297
00:10:25.090 --> 00:10:26.380
再重申一下

298
00:10:26.540 --> 00:10:27.720
在为了训练分类器

299
00:10:27.840 --> 00:10:29.380
运行你的算法

300
00:10:29.690 --> 00:10:30.840
做很多次梯度下降

301
00:10:31.140 --> 00:10:32.560
或高级优化算法的迭代之前

302
00:10:32.670 --> 00:10:33.690
要确定你

303
00:10:33.890 --> 00:10:34.990
不再使用

304
00:10:35.820 --> 00:10:37.140
梯度检验的程序

305
00:10:37.980 --> 00:10:39.120
具体来说

306
00:10:39.290 --> 00:10:40.830
如果你在每次的梯度下降法迭代时

307
00:10:41.340 --> 00:10:43.710
都运行数值梯度检验

308
00:10:44.040 --> 00:10:44.650
或者你用在

309
00:10:44.850 --> 00:10:45.780
代价函数的内循环里

310
00:10:46.670 --> 00:10:47.910
你的程序会变得非常慢

311
00:10:48.240 --> 00:10:49.860
因为数值梯度检验程序

312
00:10:50.180 --> 00:10:51.690
比反向传播算法

313
00:10:51.900 --> 00:10:53.960
要慢很多

314
00:10:54.160 --> 00:10:56.160
反向传播算法

315
00:10:56.340 --> 00:10:57.650
就是我们计算

316
00:10:58.000 --> 00:10:59.820
δ(4) δ(3) δ(2) 等等的

317
00:10:59.900 --> 00:11:02.470
那就是反向传播算法

318
00:11:02.990 --> 00:11:05.770
那是一个比梯度检验更快的计算导数的方法

319
00:11:06.620 --> 00:11:08.400
所以当你准备好了

320
00:11:08.620 --> 00:11:10.190
一旦你验证了

321
00:11:10.480 --> 00:11:12.140
反向传播的实现是正确的

322
00:11:12.220 --> 00:11:13.050
要确定你在训练算法时把它关闭了

323
00:11:13.640 --> 00:11:15.070
或者说不再使用梯度检验程序

324
00:11:15.270 --> 00:11:17.880
否则你的程序会运行得非常慢

325
00:11:20.420 --> 00:11:22.470
所以如果你计算用数值方法计算导数

326
00:11:23.110 --> 00:11:24.300
那是你用来确定反向传播实现

327
00:11:24.420 --> 00:11:26.300
是否正确的的方法

328
00:11:27.230 --> 00:11:29.290
当我实现反向传播

329
00:11:29.450 --> 00:11:31.130
或者类似的复杂模型的梯度下降算法

330
00:11:31.250 --> 00:11:33.410
我经常使用梯度检验

331
00:11:33.730 --> 00:11:36.230
这的确能帮我确定我的代码是正确的 【教育无边界字幕组】翻译：狂奔的扣肉 校对：竹二个 审核：所罗门捷列夫