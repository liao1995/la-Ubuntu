WEBVTT

1
00:00:00.160 --> 00:00:01.480
针对逻辑回归问题

2
00:00:02.110 --> 00:00:04.730
我们在之前的课程已经学习过两种优化算法

3
00:00:05.190 --> 00:00:06.190
我们首先学习了

4
00:00:06.560 --> 00:00:09.210
使用梯度下降法来优化代价函数 J(θ)

5
00:00:09.690 --> 00:00:10.770
接下来学习了

6
00:00:11.120 --> 00:00:12.730
正则化技术

7
00:00:13.520 --> 00:00:14.670
这些高级优化算法

8
00:00:14.790 --> 00:00:16.300
需要你自己设计

9
00:00:16.940 --> 00:00:18.160
代价函数 J(θ)

10
00:00:18.420 --> 00:00:20.920
自己计算导数

11
00:00:22.450 --> 00:00:23.920
在本节课中

12
00:00:24.190 --> 00:00:25.420
我们将展示

13
00:00:25.500 --> 00:00:27.570
如何改进梯度下降法和

14
00:00:27.720 --> 00:00:29.350
高级优化算法

15
00:00:30.280 --> 00:00:31.770
使其能够应用于

16
00:00:31.950 --> 00:00:33.550
正则化的逻辑回归

17
00:00:35.430 --> 00:00:36.670
接下来我们来学习其中的原理

18
00:00:37.260 --> 00:00:38.770
在之前的课程中我们注意到

19
00:00:39.190 --> 00:00:40.490
对于逻辑回归问题

20
00:00:40.850 --> 00:00:42.540
有可能会出现过拟合的现象

21
00:00:42.810 --> 00:00:44.090
如果你使用了

22
00:00:44.290 --> 00:00:45.890
类似这样的高阶多项式

23
00:00:46.470 --> 00:00:48.250
g 是 S 型函数

24
00:00:48.480 --> 00:00:49.970
具体来说

25
00:00:50.030 --> 00:00:51.330
最后你会得到这样的结果

26
00:00:51.530 --> 00:00:53.020
最后你会得到这样的结果

27
00:00:53.150 --> 00:00:54.120
分类边界看起来是一个

28
00:00:54.360 --> 00:00:55.930
过于复杂并且

29
00:00:56.620 --> 00:00:58.600
十分扭曲的函数

30
00:00:58.820 --> 00:00:59.680
针对这个训练点集

31
00:00:59.790 --> 00:01:01.000
这显然不是一个好的结果

32
00:01:01.350 --> 00:01:02.990
通常情况下

33
00:01:03.120 --> 00:01:04.890
如果要解决的逻辑回归问题有很多参数

34
00:01:05.150 --> 00:01:06.630
并且又用了过多的多项式项

35
00:01:06.790 --> 00:01:07.510
这些项大部分都是没有必要的

36
00:01:07.670 --> 00:01:09.720
最终都可能出现过拟合的现象

37
00:01:11.620 --> 00:01:14.010
这是逻辑回归问题的代价函数

38
00:01:14.810 --> 00:01:16.210
为了将其修改为正则化形式

39
00:01:16.740 --> 00:01:18.820
为了将其修改为正则化形式

40
00:01:18.950 --> 00:01:20.630
我们只需要在后面增加一项

41
00:01:20.820 --> 00:01:22.290
我们只需要在后面增加一项

42
00:01:22.650 --> 00:01:24.860
加上 λ/2m

43
00:01:25.110 --> 00:01:26.580
再跟过去一样

44
00:01:26.730 --> 00:01:29.670
这个求和将 j 从1开始

45
00:01:29.800 --> 00:01:31.000
而不是从0开始

46
00:01:31.550 --> 00:01:33.670
累积 θj 的平方

47
00:01:34.330 --> 00:01:35.470
增加的这一项

48
00:01:35.750 --> 00:01:36.960
将惩罚参数 θ1, θ2 等等

49
00:01:37.650 --> 00:01:39.140
一直到 θn

50
00:01:39.570 --> 00:01:42.600
防止这些参数取值过大

51
00:01:43.610 --> 00:01:44.720
增加了这一项之后

52
00:01:45.720 --> 00:01:46.450
产生的效果是

53
00:01:46.750 --> 00:01:48.870
即使用有很多参数的

54
00:01:49.250 --> 00:01:51.500
高阶多项式来拟合

55
00:01:52.210 --> 00:01:53.240
只要使用了正则化方法

56
00:01:53.910 --> 00:01:55.090
约束这些参数使其取值很小

57
00:01:55.850 --> 00:01:57.580
你仍有可能得到一条

58
00:01:58.830 --> 00:02:00.040
看起来是这样的分类边界

59
00:02:00.320 --> 00:02:01.460
显然  这条边界更合理地

60
00:02:02.500 --> 00:02:03.740
分开了正样本和负样本

61
00:02:05.300 --> 00:02:06.970
因此  在使用了正则化方法以后

62
00:02:08.140 --> 00:02:09.080
即使你的问题有很多参数

63
00:02:09.220 --> 00:02:11.110
正则化方法可以帮你

64
00:02:11.620 --> 00:02:13.500
避免过拟合的现象

65
00:02:14.740 --> 00:02:15.790
这到底是怎样实现的呢？

66
00:02:16.720 --> 00:02:18.280
首先看看以前学过的梯度下降法

67
00:02:18.710 --> 00:02:20.380
这是我们之前得到的更新式

68
00:02:20.670 --> 00:02:22.300
我们利用这个式子

69
00:02:22.750 --> 00:02:24.610
迭代更新 θj

70
00:02:24.740 --> 00:02:26.940
这一页幻灯片看起来和上一节课的线性回归问题很像

71
00:02:27.510 --> 00:02:28.460
但是这里我将

72
00:02:29.210 --> 00:02:31.390
θ0 的更新公式单独写出来

73
00:02:31.670 --> 00:02:32.930
第一行用来更新 θ0

74
00:02:33.060 --> 00:02:34.110
第一行用来更新 θ0

75
00:02:34.230 --> 00:02:35.470
第二行用来更新

76
00:02:35.590 --> 00:02:36.730
 θ1 到 θn

77
00:02:36.880 --> 00:02:38.470
 θ1 到 θn

78
00:02:38.900 --> 00:02:40.740
将 θ0 单独处理

79
00:02:41.700 --> 00:02:43.140
为了按照

80
00:02:43.700 --> 00:02:45.370
正则化代价函数的形式

81
00:02:46.770 --> 00:02:48.480
来修改算法

82
00:02:49.100 --> 00:02:50.510
接下来的推导

83
00:02:50.950 --> 00:02:51.810
 非常类似于

84
00:02:51.930 --> 00:02:53.700
上一节学习过的正则化线性回归

85
00:02:53.870 --> 00:02:55.620
只需要将第二个式子

86
00:02:55.890 --> 00:02:57.480
修改成这样

87
00:02:58.510 --> 00:02:59.800
我们又一次发现

88
00:03:00.380 --> 00:03:02.080
修改后的式子表面上看起来

89
00:03:02.230 --> 00:03:03.720
与上一节的线性回归问题很相似

90
00:03:04.580 --> 00:03:05.580
但是实质上这与

91
00:03:05.660 --> 00:03:06.590
我们上节学过的算法并不一样

92
00:03:06.890 --> 00:03:08.370
因为现在的假设 h(x)

93
00:03:08.780 --> 00:03:10.420
是按照这个式子定义的

94
00:03:10.860 --> 00:03:12.550
这与上一节正则化线性回归算法

95
00:03:13.130 --> 00:03:14.390
中的定义并不一样

96
00:03:14.830 --> 00:03:16.340
由于假设的不同

97
00:03:16.940 --> 00:03:18.360
我写下的迭代公式

98
00:03:18.630 --> 00:03:20.160
只是表面上看起来很像

99
00:03:20.350 --> 00:03:22.130
上一节学过的

100
00:03:22.480 --> 00:03:25.310
正则化线性回归问题中的梯度下降算法

101
00:03:26.690 --> 00:03:27.720
总结一下

102
00:03:27.830 --> 00:03:29.360
总结一下

103
00:03:29.560 --> 00:03:30.860
方括号中的这一项

104
00:03:31.130 --> 00:03:32.330
方括号中的这一项

105
00:03:32.670 --> 00:03:35.120
这一项是

106
00:03:35.410 --> 00:03:36.750
新的代价函数 J(θ)

107
00:03:37.210 --> 00:03:38.590
关于 θj 的偏导数

108
00:03:38.660 --> 00:03:41.420
关于 θj 的偏导数

109
00:03:42.300 --> 00:03:43.480
这里的 J(θ)

110
00:03:43.700 --> 00:03:44.980
是我们在上一页幻灯片中

111
00:03:45.180 --> 00:03:48.100
定义的 使用了正则化的代价函数

112
00:03:49.770 --> 00:03:52.060
以上就是正则化逻辑回归问题的梯度下降算法

113
00:03:55.200 --> 00:03:56.430
接下来我们讨论

114
00:03:56.580 --> 00:03:58.290
如何在更高级的优化算法中

115
00:03:58.950 --> 00:04:00.010
使用同样的

116
00:04:00.360 --> 00:04:02.070
正则化技术

117
00:04:03.180 --> 00:04:05.590
提醒一下

118
00:04:05.840 --> 00:04:06.800
对于这些高级算法

119
00:04:07.080 --> 00:04:08.390
我们需要自己定义

120
00:04:08.450 --> 00:04:09.460
costFuntion 函数

121
00:04:09.640 --> 00:04:11.160
这个函数有一个输入参数

122
00:04:11.280 --> 00:04:13.660
向量 theta

123
00:04:13.790 --> 00:04:16.180
theta 的内容是这样的

124
00:04:16.770 --> 00:04:19.030
我们的参数索引依然从0开始

125
00:04:19.510 --> 00:04:20.690
即 θ0 到 θn

126
00:04:21.180 --> 00:04:22.810
但是由于 Octave 中

127
00:04:23.020 --> 00:04:25.920
向量索引是从1开始

128
00:04:26.820 --> 00:04:28.240
我们的参数是从 θ0 到 θn

129
00:04:28.560 --> 00:04:29.990
在 Octave 里 是从 theta(1) 开始标号的

130
00:04:30.120 --> 00:04:31.630
而 θ1 将被记为 theta(2)

131
00:04:31.860 --> 00:04:32.930
以此类推

132
00:04:33.280 --> 00:04:35.070
直到 θn 被记为

133
00:04:36.270 --> 00:04:36.650
 theta(n+1)

134
00:04:36.740 --> 00:04:38.450
而我们需要做的

135
00:04:38.600 --> 00:04:40.240
就是将这个自定义代价函数

136
00:04:41.170 --> 00:04:42.370
这个 costFunction 函数

137
00:04:42.780 --> 00:04:44.140
代入到我们之前学过的

138
00:04:44.360 --> 00:04:46.920
代入到我们之前学过的

139
00:04:47.300 --> 00:04:48.490
fminunc函数中

140
00:04:49.060 --> 00:04:50.310
括号里面是 @costFunction

141
00:04:50.540 --> 00:04:52.160
将 @costFunction 作为参数代进去

142
00:04:54.830 --> 00:04:55.430
等等

143
00:04:55.600 --> 00:04:56.870
fminunc返回的是

144
00:04:57.030 --> 00:04:58.060
函数 costFunction 

145
00:04:58.280 --> 00:04:59.310
在无约束条件下的最小值

146
00:04:59.650 --> 00:05:01.230
因此  这个式子

147
00:05:01.310 --> 00:05:02.300
将求得代价函数的最小值

148
00:05:02.540 --> 00:05:04.340
将求得代价函数的最小值

149
00:05:05.950 --> 00:05:07.050
因此 costFunction 函数

150
00:05:07.170 --> 00:05:08.600
有两个返回值

151
00:05:08.700 --> 00:05:10.620
第一个是 jVal

152
00:05:11.280 --> 00:05:12.400
为此  我们要在这里

153
00:05:12.720 --> 00:05:13.950
补充代码

154
00:05:14.020 --> 00:05:15.710
来计算代价函数 J(θ)

155
00:05:17.130 --> 00:05:19.030
由于我们在这使用的是正则化逻辑回归

156
00:05:19.450 --> 00:05:20.920
因此

157
00:05:20.990 --> 00:05:21.960
代价函数 J(θ) 也相应需要改变

158
00:05:22.280 --> 00:05:23.450
具体来说

159
00:05:24.480 --> 00:05:25.760
代价函数需要

160
00:05:25.870 --> 00:05:29.580
增加这一正则化项

161
00:05:29.850 --> 00:05:30.930
因此  当你在计算 J(θ) 时

162
00:05:31.030 --> 00:05:33.410
需要确保包含了最后这一项

163
00:05:34.590 --> 00:05:35.520
另外 代价函数的

164
00:05:36.050 --> 00:05:37.240
另一项返回值是

165
00:05:37.690 --> 00:05:39.010
对应的梯度导数

166
00:05:39.530 --> 00:05:41.170
梯度的第一个元素

167
00:05:41.400 --> 00:05:42.570
gradient(1) 就等于

168
00:05:42.660 --> 00:05:44.080
J(θ) 关于 θ0 的偏导数

169
00:05:44.240 --> 00:05:45.520
J(θ)关于θ0的偏导数

170
00:05:45.690 --> 00:05:47.170
梯度的第二个元素按照这个式子计算

171
00:05:47.580 --> 00:05:49.520
剩余元素以此类推

172
00:05:49.780 --> 00:05:50.900
再次强调 向量元素索引是从1开始

173
00:05:51.220 --> 00:05:52.850
这是因为 Octave 的向量索引

174
00:05:53.110 --> 00:05:54.450
就是从1开始的

175
00:05:55.940 --> 00:05:56.780
再来总结一下

176
00:05:57.850 --> 00:05:58.680
首先看第一个公式

177
00:05:59.410 --> 00:06:00.640
在之前的课程中

178
00:06:00.720 --> 00:06:02.840
我们已经计算过它等于这个式子

179
00:06:03.230 --> 00:06:03.640
这个式子没有变化

180
00:06:04.120 --> 00:06:07.250
因为相比没有正则化的版本

181
00:06:07.650 --> 00:06:09.540
J(θ) 关于 θ0 的偏导数不会改变

182
00:06:10.960 --> 00:06:13.210
但是其他的公式确实有变化

183
00:06:13.840 --> 00:06:16.340
以 θ1 的偏导数为例

184
00:06:17.010 --> 00:06:18.830
在之前的课程里我们也计算过这一项

185
00:06:19.110 --> 00:06:20.670
它等于这个式子

186
00:06:20.890 --> 00:06:22.560
减去 λ 除以 m (这里应为加 校对者注)

187
00:06:23.450 --> 00:06:24.870
再乘以 θ1

188
00:06:25.310 --> 00:06:27.140
注意要确保这段代码编写正确

189
00:06:27.800 --> 00:06:29.370
建议在这里添加括号

190
00:06:29.830 --> 00:06:30.980
防止求和符号的作用域扩大

191
00:06:31.570 --> 00:06:33.160
与此类似

192
00:06:33.380 --> 00:06:34.800
再来看这个式子

193
00:06:35.130 --> 00:06:36.180
相比于之前的幻灯片

194
00:06:37.070 --> 00:06:37.950
这里多了额外的一项

195
00:06:38.030 --> 00:06:39.770
这就是正则化后的

196
00:06:39.950 --> 00:06:41.450
梯度计算方法

197
00:06:42.230 --> 00:06:43.650
当你自己定义了

198
00:06:43.820 --> 00:06:45.140
costFunction 函数

199
00:06:45.720 --> 00:06:47.370
并将其传递到 fminuc

200
00:06:48.190 --> 00:06:49.160
或者其他类似的高级优化函数中

201
00:06:50.050 --> 00:06:51.940
就可以求出

202
00:06:52.540 --> 00:06:55.990
这个新的正则化代价函数的极小值

203
00:06:56.990 --> 00:06:58.220
而返回的参数值

204
00:06:59.530 --> 00:07:00.740
即是对应的

205
00:07:01.450 --> 00:07:02.940
逻辑回归问题的正则化解

206
00:07:04.410 --> 00:07:05.540
好的 现在你知道了

207
00:07:05.780 --> 00:07:08.210
解决正则化逻辑回归问题的方法

208
00:07:09.780 --> 00:07:10.920
你知道吗 我住在硅谷

209
00:07:11.380 --> 00:07:12.900
当我在硅谷晃悠时

210
00:07:13.100 --> 00:07:14.900
我看到许多工程师

211
00:07:15.420 --> 00:07:16.490
运用机器学习算法

212
00:07:16.610 --> 00:07:18.090
给他们公司挣来了很多金子

213
00:07:19.180 --> 00:07:20.390
课讲到这里

214
00:07:20.600 --> 00:07:22.860
大家对机器学习算法可能还只是略懂

215
00:07:23.620 --> 00:07:25.410
但是一旦你精通了

216
00:07:26.510 --> 00:07:28.360
线性回归、高级优化算法

217
00:07:29.210 --> 00:07:30.710
和正则化技术

218
00:07:30.950 --> 00:07:32.520
坦率地说

219
00:07:32.950 --> 00:07:34.270
你对机器学习的理解

220
00:07:35.010 --> 00:07:36.290
可能已经比许多工程师深入了

221
00:07:36.750 --> 00:07:38.050
现在  你已经有了

222
00:07:38.180 --> 00:07:39.580
丰富的机器学习知识

223
00:07:40.240 --> 00:07:41.670
目测比那些硅谷工程师还厉害

224
00:07:41.820 --> 00:07:44.760
而那些工程师都混得还不错

225
00:07:45.300 --> 00:07:46.420
给他们公司挣了大钱 你懂的

226
00:07:47.050 --> 00:07:49.250
或者用机器学习算法来做产品

227
00:07:50.370 --> 00:07:50.960
所以 恭喜你

228
00:07:52.080 --> 00:07:53.120
你已经历练得差不多了

229
00:07:53.490 --> 00:07:54.550
已经具备足够的知识

230
00:07:54.780 --> 00:07:55.990
足够将这些算法

231
00:07:56.310 --> 00:07:58.210
用于解决实际问题

232
00:07:59.260 --> 00:08:00.580
所以你可以小小的骄傲一下了

233
00:08:00.780 --> 00:08:01.880
但是

234
00:08:02.350 --> 00:08:03.280
我还是有很多可以教你们的

235
00:08:03.400 --> 00:08:05.180
我还是有很多可以教你们的

236
00:08:05.380 --> 00:08:06.540
接下来的课程中

237
00:08:06.560 --> 00:08:07.850
我们将学习

238
00:08:08.030 --> 00:08:10.890
一个非常强大的非线性分类器

239
00:08:11.680 --> 00:08:13.350
无论是线性回归问题

240
00:08:13.690 --> 00:08:14.940
还是逻辑回归问题

241
00:08:15.080 --> 00:08:17.310
都可以构造多项式来解决

242
00:08:17.460 --> 00:08:18.350
但是 你将逐渐发现还有

243
00:08:18.510 --> 00:08:21.150
更强大的非线性分类器

244
00:08:21.460 --> 00:08:23.650
可以用来解决多项式回归问题

245
00:08:24.640 --> 00:08:25.780
在下一节课

246
00:08:25.810 --> 00:08:28.280
我将向大家介绍它们

247
00:08:28.510 --> 00:08:29.560
你将学会

248
00:08:29.760 --> 00:08:30.440
比你现在解决问题的方法

249
00:08:31.380 --> 00:08:32.870
强大N倍的学习算法 【教育无边界字幕组】翻译：伽罗  校对：所罗门捷列夫 审核：Naplessss