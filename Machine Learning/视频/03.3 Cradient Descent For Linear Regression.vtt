WEBVTT

1
00:00:00.454 --> 00:00:02.208
在以前的视频中我们谈到

2
00:00:02.238 --> 00:00:04.581
关于梯度下降算法

3
00:00:04.581 --> 00:00:06.005
梯度下降是很常用的算法 它不仅被用在线性回归上

4
00:00:06.005 --> 00:00:09.071
和线性回归模型、平方误差代价函数

5
00:00:09.071 --> 00:00:10.822
在这段视频中  我们要

6
00:00:10.822 --> 00:00:12.695
将梯度下降

7
00:00:12.695 --> 00:00:14.672
和代价函数结合

8
00:00:14.672 --> 00:00:16.652
在后面的视频中 我们将用到此算法 并将其应用于

9
00:00:16.652 --> 00:00:19.431
具体的拟合直线的线性回归算法里

10
00:00:21.001 --> 00:00:22.743
这就是

11
00:00:22.743 --> 00:00:25.077
我们在之前的课程里所做的工作

12
00:00:25.077 --> 00:00:27.095
这是梯度下降法

13
00:00:27.095 --> 00:00:29.197
这个算法你应该很熟悉

14
00:00:29.197 --> 00:00:31.199
这是线性回归模型

15
00:00:31.199 --> 00:00:36.461
还有线性假设和平方误差代价函数

16
00:00:36.461 --> 00:00:38.612
我们将要做的就是

17
00:00:38.612 --> 00:00:42.288
用梯度下降的方法

18
00:00:44.519 --> 00:00:47.537
来最小化平方误差代价函数

19
00:00:47.891 --> 00:00:49.381
为了

20
00:00:49.381 --> 00:00:50.896
使梯度下降 为了

21
00:00:50.896 --> 00:00:52.695
写这段代码

22
00:00:52.695 --> 00:00:54.191
我们需要的关键项

23
00:00:54.191 --> 00:00:58.384
是这里这个微分项

24
00:00:59.692 --> 00:01:00.798
所以.我们需要弄清楚

25
00:01:00.798 --> 00:01:02.830
这个偏导数项是什么

26
00:01:02.830 --> 00:01:04.478
并结合这里的

27
00:01:04.478 --> 00:01:06.249
代价函数J

28
00:01:06.249 --> 00:01:08.418
的定义

29
00:01:08.418 --> 00:01:11.074
就是这样

30
00:01:12.613 --> 00:01:15.156
一个求和项

31
00:01:15.156 --> 00:01:18.856
代价函数就是

32
00:01:20.456 --> 00:01:22.023
这个误差平方项

33
00:01:22.023 --> 00:01:23.794
我这样做 只是

34
00:01:23.794 --> 00:01:25.538
把定义好的代价函数

35
00:01:25.538 --> 00:01:28.275
插入了这个微分式

36
00:01:28.275 --> 00:01:30.563
再简化一下

37
00:01:30.563 --> 00:01:34.136
这等于是

38
00:01:34.136 --> 00:01:36.983
这一个求和项

39
00:01:36.983 --> 00:01:40.609
θ0 + θ1x(1) - y(i)

40
00:01:41.163 --> 00:01:43.427
θ0 + θ1x(1) - y(i)

41
00:01:43.427 --> 00:01:44.777
这一项其实就是

42
00:01:44.777 --> 00:01:46.983
我的假设的定义

43
00:01:46.983 --> 00:01:49.376
然后把这一项放进去

44
00:01:49.622 --> 00:01:51.669
实际上我们需要

45
00:01:51.669 --> 00:01:52.523
弄清楚这两个

46
00:01:52.523 --> 00:01:54.011
偏导数项是什么

47
00:01:54.011 --> 00:01:55.284
这两项分别是 j=0

48
00:01:55.284 --> 00:01:57.006
和j=1的情况

49
00:01:57.006 --> 00:01:58.547
因此我们要弄清楚

50
00:01:58.547 --> 00:02:00.767
θ0 和 θ1 对应的

51
00:02:00.767 --> 00:02:04.115
偏导数项是什么

52
00:02:04.115 --> 00:02:07.012
我只把答案写出来

53
00:02:07.012 --> 00:02:10.996
事实上 第一项可简化为

54
00:02:10.996 --> 00:02:14.218
1 / m 乘以求和式

55
00:02:14.218 --> 00:02:16.446
对所有训练样本求和

56
00:02:16.446 --> 00:02:21.146
求和项是 h(x(i))-y(i)

57
00:02:21.146 --> 00:02:23.951
而这一项

58
00:02:23.951 --> 00:02:26.186
对θ(1)的微分项

59
00:02:26.186 --> 00:02:34.954
得到的是这样一项

60
00:02:35.031 --> 00:02:36.187
对吧

61
00:02:36.402 --> 00:02:38.690
计算出这些

62
00:02:38.690 --> 00:02:40.761
偏导数项

63
00:02:40.761 --> 00:02:43.406
从这个等式

64
00:02:43.406 --> 00:02:46.414
到下面的等式

65
00:02:46.414 --> 00:02:51.090
计算这些偏导数项需要一些多元微积分

66
00:02:51.090 --> 00:02:53.118
如果你掌握了微积分

67
00:02:53.118 --> 00:02:54.825
你可以随便自己推导这些

68
00:02:54.825 --> 00:02:57.060
然后你检查你的微分

69
00:02:57.060 --> 00:02:59.853
你实际上会得到我给出的答案

70
00:02:59.853 --> 00:03:00.855
但如果你

71
00:03:00.855 --> 00:03:02.611
不太熟悉微积分

72
00:03:02.611 --> 00:03:04.235
别担心

73
00:03:04.235 --> 00:03:06.260
你可以直接用这些

74
00:03:06.260 --> 00:03:08.025
已经算出来的结果

75
00:03:08.025 --> 00:03:09.462
你不需要掌握微积分

76
00:03:09.462 --> 00:03:10.340
或者别的东西

77
00:03:10.340 --> 00:03:14.551
来完成作业 你只需要会用梯度下降就可以

78
00:03:14.551 --> 00:03:16.520
在定义这些以后

79
00:03:16.520 --> 00:03:18.156
在我们算出

80
00:03:18.156 --> 00:03:19.993
这些微分项以后

81
00:03:19.993 --> 00:03:21.316
这些微分项

82
00:03:21.316 --> 00:03:22.889
实际上就是代价函数J的斜率

83
00:03:22.889 --> 00:03:24.724
现在可以将它们放回

84
00:03:24.724 --> 00:03:27.157
我们的梯度下降算法

85
00:03:27.157 --> 00:03:28.794
所以这就是专用于

86
00:03:28.794 --> 00:03:30.309
线性回归的梯度下降

87
00:03:30.309 --> 00:03:32.971
反复执行括号中的式子直到收敛

88
00:03:32.971 --> 00:03:35.552
θ0和θ1不断被更新

89
00:03:35.552 --> 00:03:37.163
都是加上一个-α/m

90
00:03:37.163 --> 00:03:39.591
乘上后面的求和项

91
00:03:39.591 --> 00:03:43.202
所以这里这一项

92
00:03:43.202 --> 00:03:46.854
所以这就是我们的线性回归算法

93
00:03:46.854 --> 00:03:52.696
这里的第一项

94
00:03:52.696 --> 00:03:54.495
当然

95
00:03:54.495 --> 00:03:56.071
这一项就是关于θ0的偏导数

96
00:03:56.071 --> 00:03:59.995
在上一张幻灯片中推出的

97
00:03:59.995 --> 00:04:03.454
而第二项

98
00:04:03.454 --> 00:04:04.199
这一项是刚刚的推导出的

99
00:04:04.199 --> 00:04:05.947
关于θ1的

100
00:04:05.947 --> 00:04:11.188
偏导数项

101
00:04:11.188 --> 00:04:13.582
提醒一下

102
00:04:13.582 --> 00:04:15.485
执行梯度下降时

103
00:04:15.485 --> 00:04:17.067
有一个细节要注意

104
00:04:17.067 --> 00:04:19.248
就是必须要

105
00:04:19.248 --> 00:04:24.452
同时更新θ0和θ1

106
00:04:24.452 --> 00:04:28.270
所以 让我们来看看梯度下降是如何工作的

107
00:04:28.270 --> 00:04:29.447
我们用梯度下降解决问题的

108
00:04:29.447 --> 00:04:32.839
一个原因是 它更容易得到局部最优值

109
00:04:32.839 --> 00:04:34.433
当我第一次解释梯度下降时

110
00:04:34.449 --> 00:04:36.136
我展示过这幅图

111
00:04:36.136 --> 00:04:37.724
在表面上

112
00:04:37.724 --> 00:04:38.788
不断下降

113
00:04:38.788 --> 00:04:40.120
并且我们知道了

114
00:04:40.120 --> 00:04:42.872
根据你的初始化 你会得到不同的局部最优解

115
00:04:42.872 --> 00:04:44.846
你知道.你可以结束了.在这里或这里。

116
00:04:44.846 --> 00:04:46.958
但是 事实证明

117
00:04:46.958 --> 00:04:49.025
用于线性回归的

118
00:04:49.025 --> 00:04:50.791
代价函数

119
00:04:50.791 --> 00:04:52.754
总是这样一个

120
00:04:52.754 --> 00:04:55.305
弓形的样子

121
00:04:55.305 --> 00:04:57.573
这个函数的专业术语是

122
00:04:57.573 --> 00:05:01.163
这是一个凸函数

123
00:05:02.825 --> 00:05:04.920
我不打算在这门课中

124
00:05:04.920 --> 00:05:06.561
给出凸函数的定义

125
00:05:06.561 --> 00:05:09.557
凸函数(convex function)

126
00:05:09.557 --> 00:05:11.310
但不正式的说法是

127
00:05:11.310 --> 00:05:14.793
它就是一个弓形的函数

128
00:05:14.793 --> 00:05:18.006
因此 这个函数

129
00:05:18.006 --> 00:05:19.738
没有任何局部最优解

130
00:05:19.738 --> 00:05:22.433
只有一个全局最优解

131
00:05:22.433 --> 00:05:24.265
并且无论什么时候

132
00:05:24.265 --> 00:05:25.590
你对这种代价函数

133
00:05:25.590 --> 00:05:27.695
使用线性回归

134
00:05:27.695 --> 00:05:29.201
梯度下降法得到的结果

135
00:05:29.201 --> 00:05:33.623
总是收敛到全局最优值 因为没有全局最优以外的其他局部最优点

136
00:05:33.623 --> 00:05:37.272
现在 让我们来看看这个算法的执行过程

137
00:05:38.026 --> 00:05:40.085
像往常一样

138
00:05:40.085 --> 00:05:42.182
这是假设函数的图

139
00:05:42.182 --> 00:05:45.024
还有代价函数J的图

140
00:05:45.763 --> 00:05:47.011
让我们来看看如何

141
00:05:47.011 --> 00:05:49.687
初始化参数的值

142
00:05:49.687 --> 00:05:51.652
通常来说

143
00:05:51.652 --> 00:05:53.590
初始化参数为零

144
00:05:53.590 --> 00:05:56.287
θ0和θ1都在零

145
00:05:56.287 --> 00:05:58.331
但为了展示需要

146
00:05:58.331 --> 00:05:59.948
在这个梯度下降的实现中

147
00:05:59.948 --> 00:06:02.615
我把θ0初始化为-900

148
00:06:02.615 --> 00:06:06.831
θ1初始化为-0.1

149
00:06:06.831 --> 00:06:09.791
这对应的假设

150
00:06:09.791 --> 00:06:12.022
就应该是这样

151
00:06:12.022 --> 00:06:15.859
h(x)是等于-900减0.1x

152
00:06:15.859 --> 00:06:19.341
这对应我们的代价函数

153
00:06:19.341 --> 00:06:20.491
现在 如果我们进行

154
00:06:20.491 --> 00:06:22.163
一次梯度下降

155
00:06:22.163 --> 00:06:24.298
从这个点开始

156
00:06:24.298 --> 00:06:27.065
在这里.一点点

157
00:06:27.065 --> 00:06:29.564
向左下方移动了一小步

158
00:06:29.564 --> 00:06:31.732
这就得到了第二个点

159
00:06:31.732 --> 00:06:35.279
而且你注意到这条线改变了一点点

160
00:06:35.279 --> 00:06:36.547
然后我再进行

161
00:06:36.547 --> 00:06:41.425
一步梯度下降 左边这条线又变一点

162
00:06:41.425 --> 00:06:42.376
对吧

163
00:06:42.376 --> 00:06:43.804
同样地

164
00:06:43.804 --> 00:06:47.544
我又移到代价函数上的另一个点

165
00:06:47.544 --> 00:06:48.745
再进行一步梯度下降

166
00:06:48.745 --> 00:06:50.697
我觉得我的代价项

167
00:06:50.697 --> 00:06:53.058
应该开始下降了

168
00:06:53.058 --> 00:06:55.079
所以我的参数是

169
00:06:55.079 --> 00:06:58.062
跟随着这个轨迹

170
00:06:58.062 --> 00:07:00.368
再看左边这个图

171
00:07:00.368 --> 00:07:04.025
这个表示的是假设函数h(x)

172
00:07:04.025 --> 00:07:04.912
它变得好像

173
00:07:04.912 --> 00:07:06.429
越来越拟合数据

174
00:07:06.429 --> 00:07:09.987
直到它渐渐地

175
00:07:09.987 --> 00:07:12.663
收敛到全局最小值

176
00:07:12.663 --> 00:07:16.189
这个全局最小值

177
00:07:16.189 --> 00:07:20.506
对应的假设函数 给出了最拟合数据的解

178
00:07:20.922 --> 00:07:23.605
这就是梯度下降法

179
00:07:23.605 --> 00:07:24.969
我们刚刚运行了一遍

180
00:07:24.969 --> 00:07:27.302
并且最终得到了

181
00:07:27.302 --> 00:07:31.359
房价数据的最好拟合结果

182
00:07:31.359 --> 00:07:34.108
现在你可以用它来预测

183
00:07:34.108 --> 00:07:35.284
比如说 假如你有个朋友

184
00:07:35.284 --> 00:07:36.452
他有一套房子

185
00:07:36.452 --> 00:07:39.116
面积1250平方英尺(约116平米)

186
00:07:39.116 --> 00:07:40.684
现在你可以通过这个数据

187
00:07:40.684 --> 00:07:42.090
然后告诉他们

188
00:07:42.090 --> 00:07:43.188
也许他的房子

189
00:07:43.188 --> 00:07:47.159
可以卖到35万美元

190
00:07:48.606 --> 00:07:49.982
最后 我想再给出

191
00:07:49.982 --> 00:07:51.930
另一个名字

192
00:07:51.930 --> 00:07:52.991
实际上 我们

193
00:07:52.991 --> 00:07:55.030
刚刚使用的算法

194
00:07:55.030 --> 00:07:57.074
有时也称为批量梯度下降

195
00:07:57.074 --> 00:07:58.781
实际上 在机器学习中

196
00:07:58.781 --> 00:08:00.203
我们这些搞机器学习的人

197
00:08:00.203 --> 00:08:02.050
通常不太会

198
00:08:02.050 --> 00:08:04.381
给算法起名字

199
00:08:04.381 --> 00:08:06.634
但这个名字"批量梯度下降"

200
00:08:06.634 --> 00:08:08.212
指的是

201
00:08:08.212 --> 00:08:09.551
在梯度下降的

202
00:08:09.551 --> 00:08:11.164
每一步中

203
00:08:11.164 --> 00:08:13.649
我们都用到了所有的训​​练样本

204
00:08:13.649 --> 00:08:15.783
在梯度下降中

205
00:08:15.783 --> 00:08:18.875
在计算微分求导项时

206
00:08:18.875 --> 00:08:21.307
我们需要进行求和运算

207
00:08:21.307 --> 00:08:22.210
所以 在每一个单独的

208
00:08:22.210 --> 00:08:23.510
梯度下降中 我们最终

209
00:08:23.510 --> 00:08:25.278
都要计算这样一个东西

210
00:08:25.278 --> 00:08:28.434
这个项需要对所有m个训练样本求和

211
00:08:28.434 --> 00:08:29.835
因此 批量梯度下降法

212
00:08:29.835 --> 00:08:31.195
这个名字 说明了

213
00:08:31.195 --> 00:08:33.193
我们需要考虑

214
00:08:33.193 --> 00:08:34.559
所有这一"批"训练样本

215
00:08:34.559 --> 00:08:35.742
这确实不是一个

216
00:08:35.742 --> 00:08:36.915
很好听的名字

217
00:08:36.915 --> 00:08:39.444
但是搞机器学习的人就是这么称呼的

218
00:08:39.444 --> 00:08:40.958
而事实上

219
00:08:40.958 --> 00:08:42.665
有时也有其他类型的

220
00:08:42.665 --> 00:08:43.918
梯度下降法

221
00:08:43.918 --> 00:08:45.969
不是这种"批量"型的

222
00:08:45.969 --> 00:08:47.752
不考虑整个的训练集

223
00:08:47.752 --> 00:08:49.722
而是每次只关注

224
00:08:49.722 --> 00:08:51.529
训练集中的一些小的子集

225
00:08:51.529 --> 00:08:54.874
在后面的课程中 我们也将介绍这些方法

226
00:08:54.874 --> 00:08:56.195
但就目前而言

227
00:08:56.195 --> 00:08:57.410
应用刚刚学到的算法

228
00:08:57.410 --> 00:08:58.759
你应该已经掌握了批量梯度算法

229
00:08:58.759 --> 00:09:01.159
并且能把它应用到

230
00:09:01.159 --> 00:09:04.149
线性回归中了

231
00:09:05.856 --> 00:09:08.672
这就是用于线性回归的梯度下降法

232
00:09:09.349 --> 00:09:11.747
如果你之前学过线性代数

233
00:09:11.747 --> 00:09:12.672
有些同学之前

234
00:09:12.672 --> 00:09:14.206
可能已经学过

235
00:09:14.206 --> 00:09:16.279
高等线性代数

236
00:09:16.295 --> 00:09:17.678
你应该知道

237
00:09:17.678 --> 00:09:19.754
有一种计算

238
00:09:19.754 --> 00:09:20.914
代价函数J最小值的

239
00:09:20.914 --> 00:09:22.561
数值解法

240
00:09:22.561 --> 00:09:25.604
不需要梯度下降这种迭代算法

241
00:09:25.604 --> 00:09:27.154
在后面的课程中

242
00:09:27.154 --> 00:09:28.098
我们也会谈到这个方法

243
00:09:28.098 --> 00:09:29.753
它可以在不需要

244
00:09:29.753 --> 00:09:31.076
多步梯度下降的情况下

245
00:09:31.076 --> 00:09:33.791
也能解出代价函数J的最小值

246
00:09:33.791 --> 00:09:37.656
这是另一种称为正规方程(normal equations)的方法

247
00:09:37.656 --> 00:09:39.167
可能你之前已经

248
00:09:39.167 --> 00:09:40.141
听说过这种方法

249
00:09:40.141 --> 00:09:41.622
实际上在数据量较大的情况下

250
00:09:41.622 --> 00:09:43.774
梯度下降法比正规方程

251
00:09:43.774 --> 00:09:45.008
要更适用一些

252
00:09:45.008 --> 00:09:47.315
现在我们已经

253
00:09:47.315 --> 00:09:48.756
掌握了梯度下降

254
00:09:48.756 --> 00:09:49.922
我们可以在不同的环境中

255
00:09:49.922 --> 00:09:51.387
使用梯度下降法

256
00:09:51.387 --> 00:09:54.849
我们还将在不同的机器学习问题中大量地使用它

257
00:09:54.849 --> 00:09:57.138
所以 祝贺大家成功

258
00:09:57.138 --> 00:10:00.317
学会你的第一个机器学习算法

259
00:10:00.317 --> 00:10:02.508
我们稍后将有一些练习

260
00:10:02.508 --> 00:10:03.659
这些练习会要求你

261
00:10:03.659 --> 00:10:05.068
实现梯度下降

262
00:10:05.068 --> 00:10:07.071
希望大家能让这些算法真正地为你工作

263
00:10:07.071 --> 00:10:08.955
但在此之前

264
00:10:08.955 --> 00:10:10.587
我还想先

265
00:10:10.587 --> 00:10:11.919
在下一组视频中

266
00:10:11.919 --> 00:10:13.223
告诉你

267
00:10:13.223 --> 00:10:15.724
泛化的梯度下降

268
00:10:15.724 --> 00:10:16.935
算法 这将使

269
00:10:16.935 --> 00:10:18.403
梯度下降更加强大

270
00:10:18.403 --> 99:59:59.000
在下一段视频中我将介绍这一问题 【教育无边界字幕组】翻译：10号少年  校对：Dragonheel 审核: 所罗门捷列夫