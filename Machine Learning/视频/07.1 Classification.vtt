WEBVTT

1
00:00:00.460 --> 00:00:01.410
在这个以及接下来的几个视频中

2
00:00:01.580 --> 00:00:02.730
我想

3
00:00:02.960 --> 00:00:04.660
开始介绍分类问题

4
00:00:05.520 --> 00:00:07.000
在分类问题中 你要预测的变量 y

5
00:00:07.110 --> 00:00:08.160
是离散的值

6
00:00:08.570 --> 00:00:10.190
我们将学习一种叫做

7
00:00:10.420 --> 00:00:11.860
逻辑回归 (Logistic Regression) 的算法

8
00:00:12.410 --> 00:00:13.620
这是目前最流行

9
00:00:13.700 --> 00:00:16.560
使用最广泛的一种学习算法

10
00:00:19.770 --> 00:00:22.150
下面是一些分类问题的例子

11
00:00:23.170 --> 00:00:24.720
此前 我们谈到的电子邮件

12
00:00:25.260 --> 00:00:26.700
垃圾邮件分类

13
00:00:27.070 --> 00:00:28.260
就是一个分类问题

14
00:00:29.380 --> 00:00:32.160
另一个例子是网上交易的分类问题

15
00:00:33.080 --> 00:00:34.110
比如一个卖东西的网站

16
00:00:34.340 --> 00:00:35.530
如果你想了解

17
00:00:35.750 --> 00:00:36.740
一个实体的交易

18
00:00:37.040 --> 00:00:39.140
是不是欺诈

19
00:00:39.260 --> 00:00:40.920
或者某人是否

20
00:00:41.060 --> 00:00:42.260
在使用偷来的信用卡

21
00:00:42.580 --> 00:00:43.890
或者是盗用了别的用户的密码

22
00:00:44.560 --> 00:00:46.830
这也是分类问题

23
00:00:47.030 --> 00:00:48.220
之前我们也谈到了

24
00:00:48.410 --> 00:00:50.610
肿瘤分类问题的例子

25
00:00:51.640 --> 00:00:53.680
区别一个肿瘤是恶性的还是良性的

26
00:00:55.070 --> 00:00:56.010
在所有的这些问题中

27
00:00:56.690 --> 00:00:57.610
我们想要预测的变量

28
00:00:57.850 --> 00:00:58.870
是变量 y

29
00:00:59.290 --> 00:01:00.110
我们可以认为

30
00:01:00.420 --> 00:01:01.710
它能够取两个值

31
00:01:02.600 --> 00:01:04.120
0 或 1

32
00:01:04.340 --> 00:01:05.780
是或者不是垃圾邮件 是或者不是欺诈

33
00:01:06.620 --> 00:01:08.740
恶性或良性

34
00:01:10.490 --> 00:01:11.430
标记为0的类

35
00:01:11.810 --> 00:01:13.160
还有一个名字

36
00:01:13.810 --> 00:01:15.660
叫做负类 (negative class)

37
00:01:15.950 --> 00:01:16.920
标记为1的类

38
00:01:17.020 --> 00:01:19.350
也叫做正类 (positive class)

39
00:01:20.170 --> 00:01:21.500
因此0可能代表良性肿瘤

40
00:01:22.070 --> 00:01:23.460
1也就是说正类

41
00:01:23.850 --> 00:01:25.940
可能标记一个恶性肿瘤

42
00:01:27.090 --> 00:01:28.410
对于两种类别的分配

43
00:01:28.860 --> 00:01:29.940
垃圾邮件

44
00:01:30.050 --> 00:01:31.140
或者不是垃圾邮件 等等

45
00:01:31.330 --> 00:01:32.470
将两个类别标记为

46
00:01:32.790 --> 00:01:34.140
正类或负类

47
00:01:34.500 --> 00:01:35.950
0 或 1 是任意的

48
00:01:36.250 --> 00:01:37.840
其实怎样都可以

49
00:01:38.680 --> 00:01:39.820
但是通常

50
00:01:39.990 --> 00:01:40.970
从直觉上来讲

51
00:01:41.460 --> 00:01:43.430
负类总是表达

52
00:01:43.590 --> 00:01:44.690
缺少某样东西的意思

53
00:01:45.000 --> 00:01:47.440
比如缺少恶性肿瘤

54
00:01:47.860 --> 00:01:49.410
而 1 正类 就会表示

55
00:01:49.950 --> 00:01:52.110
存在某样我们寻找的东西

56
00:01:52.770 --> 00:01:54.340
但是哪个是负类

57
00:01:54.560 --> 00:01:55.400
哪个是正类的定义

58
00:01:55.680 --> 00:01:58.480
有时是任意的 它并不太重要

59
00:02:00.090 --> 00:02:00.980
现在 我们要开始

60
00:02:01.340 --> 00:02:03.030
研究只有两类 0 和 1

61
00:02:03.290 --> 00:02:04.540
的分类问题

62
00:02:05.480 --> 00:02:07.010
以后 我们将讨论多类别问题

63
00:02:07.440 --> 00:02:09.320
多类别问题中的变量 y

64
00:02:09.750 --> 00:02:10.960
的取值可以是

65
00:02:11.550 --> 00:02:13.120
0 1 2 和 3 或更多

66
00:02:14.220 --> 00:02:16.810
这就是所谓的多类分类问题

67
00:02:17.680 --> 00:02:18.800
但在接下来的几个视频中

68
00:02:18.950 --> 00:02:20.280
让我们从两类分类问题

69
00:02:20.660 --> 00:02:22.750
或者叫二元分类问题开始

70
00:02:23.580 --> 00:02:25.650
我们以后再关心多类的问题

71
00:02:26.980 --> 00:02:29.440
那我们怎样开发一个分类算法呢？

72
00:02:30.530 --> 00:02:31.670
下面是一个训练集的例子

73
00:02:31.750 --> 00:02:32.730
这个训练集是用来

74
00:02:34.350 --> 00:02:35.800
给一个肿瘤分类为

75
00:02:36.240 --> 00:02:37.540
恶性或者良性的

76
00:02:37.820 --> 00:02:39.260
注意 这个恶性值 (malignancy)

77
00:02:39.530 --> 00:02:41.200
只取两个值

78
00:02:41.380 --> 00:02:43.210
0也就是非(恶性) 和 1 也就是 是(恶性)

79
00:02:44.550 --> 00:02:45.650
所以拿到这个训练集

80
00:02:45.850 --> 00:02:46.970
我们可以做的一个事情是

81
00:02:47.440 --> 00:02:48.700
将一个我们已知的算法

82
00:02:49.120 --> 00:02:52.710
线性回归用于这组数据

83
00:02:53.150 --> 00:02:55.310
尝试用一条直线来拟合数据

84
00:02:56.290 --> 00:02:57.480
所以如果用一条直线

85
00:02:57.780 --> 00:02:58.760
拟合这个训练集

86
00:02:58.900 --> 00:03:00.320
你有可能得到

87
00:03:00.700 --> 00:03:03.530
看起来像这样的假设函数

88
00:03:03.700 --> 00:03:05.920
好了 这是我的假设函数

89
00:03:06.020 --> 00:03:07.890
h(x) 等于 θ 的转置乘以 x

90
00:03:08.020 --> 00:03:09.330
如果你想进行预测

91
00:03:09.570 --> 00:03:11.270
如果你想进行预测

92
00:03:11.500 --> 00:03:12.980
你可以尝试

93
00:03:13.610 --> 00:03:16.760
将分类器的输出阈值设为0.5

94
00:03:17.110 --> 00:03:19.880
这是纵轴上0.5的位置

95
00:03:21.760 --> 00:03:23.940
如果假设输出的值

96
00:03:24.330 --> 00:03:25.490
大于等于 0.5

97
00:03:25.620 --> 00:03:27.510
你就预测 y 值等于 1

98
00:03:27.860 --> 00:03:29.940
如果小于0.5 预测y等于0

99
00:03:31.070 --> 00:03:32.540
让我们看看当我们这样做的时候会发生什么

100
00:03:32.740 --> 00:03:33.900
所以让我们取 0.5

101
00:03:34.090 --> 00:03:36.670
所以 这就是阈值的位置

102
00:03:37.070 --> 00:03:39.260
就这样使用线性回归算法

103
00:03:39.920 --> 00:03:41.060
这个点右边的所有点

104
00:03:41.330 --> 00:03:42.460
我们会将它们

105
00:03:42.640 --> 00:03:43.690
全部预测为正类

106
00:03:44.280 --> 00:03:45.390
因为它们的输出值

107
00:03:45.690 --> 00:03:46.800
在纵轴上

108
00:03:47.270 --> 00:03:48.690
都是大于0.5的

109
00:03:49.340 --> 00:03:50.730
在这一点左侧

110
00:03:51.000 --> 00:03:52.260
的所有点

111
00:03:52.490 --> 00:03:54.170
我们会预测它们全部为负

112
00:03:55.660 --> 00:03:57.570
在这个特定的例子中

113
00:03:57.720 --> 00:03:59.400
看起来好像线性回归所做的

114
00:03:59.790 --> 00:04:01.870
实际上是合理的

115
00:04:02.190 --> 00:04:03.910
尽管我们感兴趣的是

116
00:04:04.140 --> 00:04:05.430
一个分类问题

117
00:04:05.500 --> 00:04:07.420
现在我们把问题稍微改一下

118
00:04:08.060 --> 00:04:09.360
让我来延长一下横轴

119
00:04:10.040 --> 00:04:11.460
让我来延长一下横轴

120
00:04:11.650 --> 00:04:12.640
假如说新增一个训练样本

121
00:04:12.990 --> 00:04:15.030
在很远的右边那里

122
00:04:16.520 --> 00:04:17.830
注意 这个额外的训练样本

123
00:04:18.170 --> 00:04:19.200
这里这个

124
00:04:19.390 --> 00:04:21.710
它实际上并没有改变什么 对不对 ?

125
00:04:22.420 --> 00:04:23.470
看一下训练集

126
00:04:23.560 --> 00:04:26.340
相当清楚 一个很好的假设是什么

127
00:04:26.890 --> 00:04:27.920
在这儿附近的某个点

128
00:04:28.000 --> 00:04:29.050
它右侧的所有点

129
00:04:29.190 --> 00:04:29.970
我们都应该预测为正

130
00:04:30.300 --> 00:04:31.280
而它左侧的所有点

131
00:04:31.480 --> 00:04:32.690
我们应该预测为负

132
00:04:33.060 --> 00:04:34.700
因为从这个训练集来看

133
00:04:34.880 --> 00:04:35.940
因为从这个训练集来看

134
00:04:36.200 --> 00:04:37.880
好像所有大于这附近的

135
00:04:37.970 --> 00:04:39.190
某个特定值的肿瘤

136
00:04:39.490 --> 00:04:41.030
都是恶性的

137
00:04:41.200 --> 00:04:42.110
 小些的肿瘤都是非恶性的

138
00:04:42.220 --> 00:04:44.660
至少对于这个训练集是这样

139
00:04:46.160 --> 00:04:47.280
但是一旦我们在这里

140
00:04:47.720 --> 00:04:49.060
增加了额外的样本

141
00:04:49.620 --> 00:04:50.660
如果你现在运行线性回归

142
00:04:51.580 --> 00:04:53.590
拟合数据得到这样一条直线

143
00:04:54.430 --> 00:04:55.630
它可能看起来像这样

144
00:04:57.890 --> 00:04:59.860
如果你现在将假设阈值设为0.5

145
00:05:02.480 --> 00:05:03.460
你最后得到的

146
00:05:04.110 --> 00:05:05.550
阈值位置大概在这里

147
00:05:06.320 --> 00:05:07.320
所以对这个点右侧的所有点

148
00:05:07.570 --> 00:05:08.790
你都将预测为正

149
00:05:08.960 --> 00:05:11.510
它左侧的都预测为负

150
00:05:14.580 --> 00:05:15.720
这样看起来

151
00:05:16.100 --> 00:05:18.500
线性回归的效果并不好 对不对

152
00:05:18.770 --> 00:05:19.840
因为 这些是我们的正样本

153
00:05:19.930 --> 00:05:22.010
这些是我们的负样本

154
00:05:23.050 --> 00:05:24.580
可以清楚地看出

155
00:05:24.800 --> 00:05:26.000
我们真的应该把两类

156
00:05:26.550 --> 00:05:28.180
从这附近的某个点分开

157
00:05:28.670 --> 00:05:30.030
但是因为在极右侧

158
00:05:30.190 --> 00:05:31.280
添加了一个训练样本

159
00:05:31.420 --> 00:05:33.340
而这个训练样本并没有真的给我们任何新的信息

160
00:05:33.770 --> 00:05:34.950
我的意思是

161
00:05:35.170 --> 00:05:36.300
应该不出意外

162
00:05:37.030 --> 00:05:39.100
这个样本的学习结果为恶性

163
00:05:40.230 --> 00:05:41.210
但不知何故 通过在这里添加一个样本

164
00:05:41.740 --> 00:05:43.420
导致线性回归算法

165
00:05:44.410 --> 00:05:45.670
用来拟合数据的直线

166
00:05:45.980 --> 00:05:47.650
从这里洋红线改变到

167
00:05:48.840 --> 00:05:50.000
从这里洋红线改变到

168
00:05:50.840 --> 00:05:51.940
这条蓝线这里

169
00:05:52.850 --> 00:05:54.770
结果使我们得到一条更不好的假设

170
00:05:56.950 --> 00:05:58.440
因此应用线性回归

171
00:05:59.080 --> 00:06:01.030
来解决分类问题通常

172
00:06:01.610 --> 00:06:03.400
不是一个好主意

173
00:06:04.430 --> 00:06:05.750
在第一个例子中

174
00:06:05.810 --> 00:06:07.090
在我追加那个

175
00:06:07.540 --> 00:06:08.780
额外的训练样本之前

176
00:06:09.810 --> 00:06:11.430
线性回归

177
00:06:11.650 --> 00:06:13.200
只是碰巧了

178
00:06:13.380 --> 00:06:14.990
得到了适用于

179
00:06:15.090 --> 00:06:16.290
这个特殊例子的假设

180
00:06:16.670 --> 00:06:19.470
但是通常来说

181
00:06:19.980 --> 00:06:20.970
对一组数据使用线性回归

182
00:06:21.820 --> 00:06:23.040
你可能会很幸运

183
00:06:23.270 --> 00:06:24.130
但往往不是一个好想法

184
00:06:24.260 --> 00:06:25.730
所以我不会用

185
00:06:25.980 --> 00:06:27.960
线性回归算法来解决分类问题

186
00:06:29.670 --> 00:06:30.820
这里还有一个有趣的事情

187
00:06:31.250 --> 00:06:32.650
如果我们要用

188
00:06:32.930 --> 00:06:35.510
线性回归算法来解决一个分类问题

189
00:06:36.690 --> 00:06:38.220
对于分类 我们知道

190
00:06:38.450 --> 00:06:39.790
y 取值为 0 或者 1

191
00:06:40.580 --> 00:06:41.620
但如果你使用的是线性回归

192
00:06:41.890 --> 00:06:43.050
那么假设函数的输出值

193
00:06:44.210 --> 00:06:45.750
可能远大于 1

194
00:06:46.060 --> 00:06:47.330
或者远小于 0

195
00:06:47.500 --> 00:06:48.820
即使所有

196
00:06:49.050 --> 00:06:50.690
训练样本的标签 y

197
00:06:51.140 --> 00:06:52.410
都等于 0 或 1

198
00:06:53.900 --> 00:06:54.880
这看起来有点儿奇怪

199
00:06:55.520 --> 00:06:56.760
尽管我们知道

200
00:06:56.960 --> 00:06:58.160
标签应该取值

201
00:06:58.350 --> 00:06:59.320
0 或者 1

202
00:06:59.420 --> 00:07:00.890
但是如果算法得到的值

203
00:07:01.210 --> 00:07:02.580
远大于1或者远小于0的话

204
00:07:02.840 --> 00:07:04.900
就会感觉很奇怪

205
00:07:09.540 --> 00:07:10.900
所以我们在接下来的

206
00:07:11.000 --> 00:07:12.400
几个视频中要研究的

207
00:07:12.860 --> 00:07:14.640
算法就叫做逻辑回归算法

208
00:07:15.550 --> 00:07:17.390
这个算法的性质是

209
00:07:17.780 --> 00:07:19.290
它的输出值永远在

210
00:07:19.670 --> 00:07:21.220
0 到 1 之间

211
00:07:21.630 --> 00:07:22.750
并不会大于 1

212
00:07:23.060 --> 00:07:24.170
也不会小于 0

213
00:07:24.370 --> 00:07:26.370
顺便说一下

214
00:07:26.530 --> 00:07:28.570
逻辑回归算法

215
00:07:29.090 --> 00:07:30.150
是分类算法

216
00:07:30.350 --> 00:07:32.770
并且我们将它作为分类算法使用

217
00:07:33.330 --> 00:07:35.060
有时候可能因为这个算法

218
00:07:35.780 --> 00:07:37.410
的名字中出现了“回归”使你感到困惑

219
00:07:37.700 --> 00:07:39.360
但逻辑回归算法

220
00:07:39.970 --> 00:07:41.280
实际上是一种分类算法

221
00:07:42.120 --> 00:07:43.040
但是这只是因为历史原因 而被这样称呼

222
00:07:43.160 --> 00:07:46.140
所以千万不要被迷惑

223
00:07:46.680 --> 00:07:48.340
逻辑回归实际上

224
00:07:48.430 --> 00:07:50.250
是一个分类算法

225
00:07:50.380 --> 00:07:52.030
它适用于标签 y 取值离散的情况

226
00:07:52.160 --> 00:07:54.780
如 1 0 0 1

227
00:07:55.820 --> 00:07:57.440
所以希望你现在

228
00:07:57.680 --> 00:07:59.180
知道为什么

229
00:07:59.280 --> 00:08:00.950
对一个分类问题

230
00:08:01.400 --> 00:08:02.660
使用线性回归算法并不是一个好主意

231
00:08:03.210 --> 00:08:04.480
在接下来的视频中

232
00:08:04.700 --> 00:08:05.680
我们将开始学习

233
00:08:06.290 --> 00:08:07.640
逻辑回归算法的细节【教育无边界字幕组】翻译：Jaminalia  校对：竹二个 审核：所罗门捷列夫