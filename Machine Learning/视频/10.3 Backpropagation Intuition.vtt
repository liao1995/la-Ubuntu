WEBVTT

1
00:00:00.260 --> 00:00:03.120
在上一段视频中 我们介绍了反向传播算法

2
00:00:04.230 --> 00:00:05.090
对很多人来说

3
00:00:05.220 --> 00:00:06.140
当第一次看到这种算法时

4
00:00:06.460 --> 00:00:07.610
第一印象通常是

5
00:00:08.070 --> 00:00:09.250
哇哦

6
00:00:09.380 --> 00:00:11.650
这个算法需要那么多繁杂的步骤

7
00:00:11.970 --> 00:00:12.990
简直是太复杂了

8
00:00:13.130 --> 00:00:13.980
实在不知道这些步骤

9
00:00:14.180 --> 00:00:15.130
到底应该如何合在一起使用

10
00:00:15.400 --> 00:00:17.830
就好像一个黑箱 里面充满了复杂的步骤

11
00:00:18.130 --> 00:00:18.830
如果你对反向传播算法

12
00:00:18.870 --> 00:00:20.460
也有这种感受的话

13
00:00:20.860 --> 00:00:22.100
这其实是正常的

14
00:00:22.740 --> 00:00:24.100
相比于线性回归算法

15
00:00:24.970 --> 00:00:26.920
和逻辑回归算法而言

16
00:00:27.060 --> 00:00:28.520
从数学的角度上讲

17
00:00:28.860 --> 00:00:30.680
反向传播算法

18
00:00:31.130 --> 00:00:32.850
似乎并不简洁

19
00:00:33.020 --> 00:00:35.560
对于反向传播这种算法

20
00:00:36.080 --> 00:00:37.310
其实我已经使用了很多年了

21
00:00:37.530 --> 00:00:39.130
但即便如此

22
00:00:39.510 --> 00:00:40.320
即使是现在 我也经常感觉

23
00:00:40.430 --> 00:00:41.790
自己对反向传播算法的理解并不是十分深入

24
00:00:42.130 --> 00:00:43.580
对于反向传播算法究竟是如何执行的

25
00:00:43.830 --> 00:00:45.980
并没有一个很直观的理解

26
00:00:46.740 --> 00:00:47.850
做过编程练习的同学

27
00:00:48.250 --> 00:00:49.920
应该可以感受到

28
00:00:50.480 --> 00:00:51.970
这些练习或多或少能帮助你

29
00:00:52.280 --> 00:00:53.710
将这些复杂的步骤梳理了一遍

30
00:00:53.810 --> 00:00:54.910
巩固了反向传播算法具体是如何实现的

31
00:00:55.200 --> 00:00:56.860
这样你才能自己掌握这种算法

32
00:00:57.910 --> 00:00:58.850
在这段视频中

33
00:00:58.970 --> 00:01:00.170
我想更加深入地

34
00:01:00.460 --> 00:01:01.750
讨论一下

35
00:01:02.190 --> 00:01:03.640
反向传播算法的这些复杂的步骤

36
00:01:04.160 --> 00:01:05.620
并且希望给你一个

37
00:01:05.840 --> 00:01:07.450
更加全面直观的感受

38
00:01:07.930 --> 00:01:09.080
理解这些步骤究竟是在做什么

39
00:01:09.250 --> 00:01:10.590
也希望通过这段视频

40
00:01:10.790 --> 00:01:12.530
你能理解 它至少还是一个合理的算法

41
00:01:14.680 --> 00:01:16.240
但可能你即使看了这段视频

42
00:01:16.380 --> 00:01:18.000
你还是觉得

43
00:01:18.760 --> 00:01:19.920
反向传播依然很复杂

44
00:01:20.160 --> 00:01:21.600
依然像一个黑箱 太多复杂的步骤

45
00:01:22.150 --> 00:01:23.230
依然感到有点神奇

46
00:01:23.330 --> 00:01:24.740
这也是没关系的

47
00:01:24.930 --> 00:01:26.760
我说了

48
00:01:27.050 --> 00:01:27.840
即使是我 接触反向传播这么多年了

49
00:01:28.070 --> 00:01:31.590
有时候仍然觉得这是一个难以理解的算法

50
00:01:32.310 --> 00:01:34.140
但还是希望这段视频能有些许帮助

51
00:01:36.410 --> 00:01:37.970
为了更好地理解

52
00:01:38.190 --> 00:01:39.660
反向传播算法

53
00:01:40.100 --> 00:01:42.290
我们再来仔细研究一下前向传播的原理

54
00:01:43.170 --> 00:01:44.420
幻灯片所示的神经网络

55
00:01:44.770 --> 00:01:46.070
包含两个输入单元

56
00:01:46.390 --> 00:01:48.480
这不包括偏差单元

57
00:01:48.700 --> 00:01:50.300
在第二层有两个隐藏单元

58
00:01:50.500 --> 00:01:51.590
在下一层也有两个隐藏单元

59
00:01:52.030 --> 00:01:53.490
最后的输出层

60
00:01:53.640 --> 00:01:55.090
有一个输出单元

61
00:01:55.520 --> 00:01:57.800
再提醒一下 这里说的2 2 2

62
00:01:57.920 --> 00:02:00.240
都不算顶上附加的偏差单元+1

63
00:02:01.520 --> 00:02:03.170
为了更清楚地展示前向传播

64
00:02:03.430 --> 00:02:04.570
我想把这个网络

65
00:02:04.690 --> 00:02:06.080
画得稍微不同一些

66
00:02:08.040 --> 00:02:09.180
具体来讲

67
00:02:09.370 --> 00:02:10.840
我把这个神经网络的节点

68
00:02:10.930 --> 00:02:12.620
都画成椭圆型

69
00:02:12.920 --> 00:02:15.010
以便在节点里面写字

70
00:02:15.840 --> 00:02:16.800
在进行前向传播时

71
00:02:17.600 --> 00:02:18.900
我们可以用一个具体的例子说明

72
00:02:19.760 --> 00:02:21.190
假如说 训练样本

73
00:02:21.610 --> 00:02:22.990
x(i) y(i)

74
00:02:23.080 --> 00:02:24.550
那么这里的 x(i)

75
00:02:24.740 --> 00:02:26.460
将被传入输入层

76
00:02:27.080 --> 00:02:28.850
因此这里就是

77
00:02:29.110 --> 00:02:30.290
x(i)1 和 x(i)2

78
00:02:30.440 --> 00:02:31.360
这是我们输入层的值

79
00:02:31.510 --> 00:02:32.870
那么

80
00:02:33.010 --> 00:02:34.350
当我们进行前向传播

81
00:02:34.650 --> 00:02:36.210
传播到第一个隐藏层时

82
00:02:36.360 --> 00:02:38.070
我们的做法是 算出 z(2)1

83
00:02:39.370 --> 00:02:42.900
和 z(2)2

84
00:02:43.770 --> 00:02:45.010
因此这两个值

85
00:02:45.260 --> 00:02:47.000
是输入单元的加权总和

86
00:02:47.230 --> 00:02:48.680
接下来

87
00:02:48.940 --> 00:02:50.670
我们将S型的逻辑函数

88
00:02:51.940 --> 00:02:53.630
和S型的激励函数

89
00:02:54.050 --> 00:02:55.670
应用到z值上

90
00:02:55.960 --> 00:02:57.520
得出了这样的激励值

91
00:02:57.880 --> 00:02:59.670
因此我们得到 a(2)1

92
00:02:59.870 --> 00:03:01.160
和 a(2)2 的值

93
00:03:01.260 --> 00:03:02.500
然后 再做一次前向传播

94
00:03:03.940 --> 00:03:05.570
这里的 z(3)1

95
00:03:06.010 --> 00:03:07.500
应用S型的逻辑函数

96
00:03:07.690 --> 00:03:09.500
和激励函数

97
00:03:10.080 --> 00:03:11.200
得到 a(3)1

98
00:03:11.240 --> 00:03:14.310
类似这样进行下去

99
00:03:15.580 --> 00:03:17.850
最后我们得到 z(4)1

100
00:03:18.080 --> 00:03:19.450
应用激励函数

101
00:03:19.630 --> 00:03:20.940
得到 a(4)1

102
00:03:21.630 --> 00:03:23.030
这也是这个网络的输出单元的值

103
00:03:24.860 --> 00:03:25.920
我把这个箭头擦掉

104
00:03:26.040 --> 00:03:28.490
这样留点书写空间

105
00:03:28.620 --> 00:03:30.170
那么

106
00:03:30.610 --> 00:03:32.280
如果你仔细看这里的计算

107
00:03:32.780 --> 00:03:33.970
关注这一层的隐藏单元

108
00:03:34.400 --> 00:03:35.860
我们知道了这个权值

109
00:03:36.090 --> 00:03:37.770
这里用桃红色表示的

110
00:03:37.870 --> 00:03:39.500
这是我们的权值

111
00:03:39.700 --> 00:03:42.820
θ(2)10

112
00:03:43.090 --> 00:03:45.930
这里的角标不重要

113
00:03:46.140 --> 00:03:47.440
而这里的权值

114
00:03:47.570 --> 00:03:49.270
我用红色来标记的

115
00:03:49.630 --> 00:03:51.290
是θ(2)11

116
00:03:52.870 --> 00:03:53.970
而这里的权值

117
00:03:54.050 --> 00:03:55.370
我用青色表示的

118
00:03:55.720 --> 00:03:59.530
是θ(2)12

119
00:04:00.410 --> 00:04:01.970
因此要计算 z(3)1

120
00:04:02.540 --> 00:04:05.230
z(3)1 的值等于

121
00:04:05.410 --> 00:04:09.120
这个桃红色的权值

122
00:04:10.430 --> 00:04:11.840
乘以这个值

123
00:04:13.070 --> 00:04:14.970
也就是θ(2)10 乘上1

124
00:04:16.240 --> 00:04:19.190
加上这个红色的权值

125
00:04:19.410 --> 00:04:21.480
乘以这个值

126
00:04:21.670 --> 00:04:23.690
也就是θ(2)11

127
00:04:25.270 --> 00:04:28.520
乘上a(2)1

128
00:04:28.860 --> 00:04:30.140
最后是青色的权值乘上这个值

129
00:04:30.660 --> 00:04:33.950
也就是

130
00:04:35.120 --> 00:04:37.300
θ(2)12乘以a(2)1

131
00:04:38.870 --> 00:04:40.170
那么这就是前向传播

132
00:04:42.410 --> 00:04:43.680
事实上

133
00:04:43.870 --> 00:04:44.730
正如我们后面将会看到的

134
00:04:44.790 --> 00:04:46.140
反向传播的做法

135
00:04:46.530 --> 00:04:47.730
其过程

136
00:04:47.780 --> 00:04:49.120
非常类似于此

137
00:04:49.300 --> 00:04:50.860
只有计算的方向不同而已

138
00:04:50.950 --> 00:04:53.120
与这里前向传播的方向从左至右

139
00:04:53.360 --> 00:04:54.270
不同的是

140
00:04:55.250 --> 00:04:56.510
反向传播的算法中

141
00:04:56.940 --> 00:04:58.070
计算的方向是

142
00:04:58.220 --> 00:04:59.720
从右往左的

143
00:05:00.050 --> 00:05:02.170
但计算的过程是完全类似的

144
00:05:02.430 --> 00:05:03.710
在接下来的两页幻灯片中

145
00:05:03.920 --> 00:05:05.260
我会详细地讲解

146
00:05:06.400 --> 00:05:07.880
为了更好地理解

147
00:05:08.070 --> 00:05:09.710
反向传播算法的原理

148
00:05:09.780 --> 00:05:10.920
我们把目光转向代价函数

149
00:05:11.070 --> 00:05:12.270
这个代价函数

150
00:05:12.670 --> 00:05:14.950
对应的情况是只有一个输出单元

151
00:05:15.350 --> 00:05:16.300
如果我们有不止一个

152
00:05:16.400 --> 00:05:17.410
输出单元的话

153
00:05:17.820 --> 00:05:19.850
只需要对所有的输出单元

154
00:05:19.930 --> 00:05:22.170
进行一次求和运算

155
00:05:22.370 --> 00:05:25.990
但如果只有一个输出单元时

156
00:05:26.190 --> 00:05:27.490
代价函数就是这样

157
00:05:27.610 --> 00:05:30.340
我们用同一个样本同时来做正向和反向传播

158
00:05:30.560 --> 00:05:31.440
那么 请注意这组训练样本

159
00:05:31.770 --> 00:05:34.770
x(i)  y(i)

160
00:05:35.360 --> 00:05:36.480
注意这种只有一个输出单元的情况

161
00:05:36.810 --> 00:05:38.390
那么这里的 y(i)

162
00:05:38.660 --> 00:05:40.390
就是一个实数

163
00:05:40.680 --> 00:05:42.790
如果不考虑正则化

164
00:05:43.010 --> 00:05:44.300
也就是说 λ 等于0

165
00:05:44.640 --> 00:05:46.480
因此最后的正则化项就没有了

166
00:05:47.320 --> 00:05:48.220
好的 那么如果你观察

167
00:05:48.730 --> 00:05:50.480
这个求和运算括号里面

168
00:05:50.780 --> 00:05:53.290
与第i个训练样本对应的

169
00:05:53.450 --> 00:05:54.980
代价项

170
00:05:55.190 --> 00:05:57.230
也就是说

171
00:05:58.040 --> 00:06:00.420
和训练样本 x(i) y(i) 对应的代价项

172
00:06:00.540 --> 00:06:01.820
将由这个式子确定

173
00:06:02.030 --> 00:06:03.270
因此 第 i 个样本的代价值

174
00:06:03.810 --> 00:06:04.910
可以写成如下的形式

175
00:06:06.080 --> 00:06:07.320
而这个代价函数

176
00:06:07.650 --> 00:06:08.650
所扮演的角色

177
00:06:09.080 --> 00:06:10.580
可以看作是平方误差

178
00:06:10.750 --> 00:06:11.530
因此 我们不必关心

179
00:06:12.190 --> 00:06:14.050
这个复杂的表达式

180
00:06:14.170 --> 00:06:15.380
当然如果你愿意

181
00:06:15.620 --> 00:06:17.600
你可以把 cost(i) 想成是

182
00:06:18.020 --> 00:06:19.310
该神经网络输出值

183
00:06:19.430 --> 00:06:20.870
与实际值的

184
00:06:21.170 --> 00:06:22.980
差的平方

185
00:06:23.150 --> 00:06:24.340
就像在逻辑回归中

186
00:06:24.620 --> 00:06:25.510
我们选择稍微复杂的一点的

187
00:06:25.830 --> 00:06:27.060
代价函数

188
00:06:27.370 --> 00:06:28.580
log函数

189
00:06:28.640 --> 00:06:30.230
但为了容易理解

190
00:06:30.570 --> 00:06:31.440
可以把这个代价函数

191
00:06:32.000 --> 00:06:32.750
看作是某种

192
00:06:33.250 --> 00:06:35.000
平方误差函数

193
00:06:35.220 --> 00:06:36.870
因此 这里的cos(i)

194
00:06:37.110 --> 00:06:38.780
表征了该神经网络

195
00:06:38.880 --> 00:06:40.600
是否能准确地预测样本i的值

196
00:06:40.840 --> 00:06:42.000
也就是输出值

197
00:06:42.810 --> 00:06:44.640
和实际观测值y(i)的接近程度

198
00:06:45.590 --> 00:06:47.610
现在我们来看反向传播是怎么做的

199
00:06:48.420 --> 00:06:50.170
一种直观的理解是

200
00:06:51.190 --> 00:06:52.940
反向传播算法就是在计算

201
00:06:53.610 --> 00:06:54.840
所有这些δ(i)j项

202
00:06:55.050 --> 00:06:57.440
并且我们可以

203
00:06:57.730 --> 00:06:58.520
把它们看作是

204
00:06:58.650 --> 00:07:00.070
这些激励值的

205
00:07:00.300 --> 00:07:02.460
"误差"

206
00:07:02.620 --> 00:07:03.980
注意这些激励值是

207
00:07:04.440 --> 00:07:05.750
第 l 层中的

208
00:07:07.130 --> 00:07:07.400
第 j 项

209
00:07:07.660 --> 00:07:09.070
更正式一点的说法是

210
00:07:09.340 --> 00:07:10.280
也许那些比较熟悉微积分的同学

211
00:07:10.360 --> 00:07:11.480
更能理解

212
00:07:12.690 --> 00:07:14.080
更正式地说

213
00:07:14.260 --> 00:07:15.820
δ 项实际上是

214
00:07:15.950 --> 00:07:17.810
关于 z(l)j 的

215
00:07:18.240 --> 00:07:20.000
偏微分

216
00:07:20.150 --> 00:07:21.460
也就是 cost 函数

217
00:07:21.650 --> 00:07:22.700
关于我们计算出的 输入项的加权和

218
00:07:23.410 --> 00:07:25.760
也就是 z 项的 偏微分

219
00:07:27.000 --> 00:07:28.650
所以 实际上这个代价函数

220
00:07:28.900 --> 00:07:30.000
是一个关于标签 y

221
00:07:30.250 --> 00:07:31.350
和这个 h(x) 的值也就是

222
00:07:31.470 --> 00:07:32.680
神经网络输出值

223
00:07:32.780 --> 00:07:35.060
的函数

224
00:07:35.180 --> 00:07:36.430
如果我们观察该网络内部的话

225
00:07:37.340 --> 00:07:39.200
把这些 z(l)j 项

226
00:07:39.860 --> 00:07:41.450
稍微改一点点

227
00:07:41.640 --> 00:07:44.250
那就将影响到该神经网络的输出

228
00:07:44.990 --> 00:07:47.290
并且最终会改变代价函数的值

229
00:07:48.340 --> 00:07:50.120
当然 还是那句话

230
00:07:50.210 --> 00:07:51.690
讲这些只是对那些熟悉微积分的同学

231
00:07:52.960 --> 00:07:55.580
如果你对偏微分很熟悉的话

232
00:07:56.540 --> 00:07:57.860
你能理解这些δ项是什么

233
00:07:57.950 --> 00:07:59.270
它们实际上是

234
00:07:59.370 --> 00:08:00.800
代价函数

235
00:08:00.870 --> 00:08:04.010
关于这些中间项的偏微分

236
00:08:05.500 --> 00:08:07.250
因此 它们度量着

237
00:08:07.910 --> 00:08:08.940
我们对神经网络的权值

238
00:08:09.140 --> 00:08:11.090
做多少的改变

239
00:08:11.250 --> 00:08:13.620
对中间的计算量

240
00:08:14.150 --> 00:08:16.110
影响是多少

241
00:08:16.240 --> 00:08:17.430
进一步地

242
00:08:17.470 --> 00:08:18.980
对整个神经网络的输出 h(x) 影响多少

243
00:08:19.160 --> 00:08:20.770
以及对整个的代价值影响多少

244
00:08:21.510 --> 00:08:22.820
可能刚才讲的

245
00:08:23.030 --> 00:08:25.290
偏微分的这种理解

246
00:08:25.530 --> 00:08:26.920
不太容易理解

247
00:08:27.070 --> 00:08:28.230
没关系

248
00:08:28.390 --> 00:08:29.770
不用偏微分的思想

249
00:08:30.280 --> 00:08:32.400
我们同样也可以理解

250
00:08:32.660 --> 00:08:33.780
我们再深入一点

251
00:08:34.100 --> 00:08:36.020
研究一下反向传播的过程

252
00:08:36.250 --> 00:08:37.440
对于输出层

253
00:08:37.890 --> 00:08:39.630
如果我们设置δ项

254
00:08:39.830 --> 00:08:41.400
比如说 δ(4)1 等于 y(i)

255
00:08:41.700 --> 00:08:44.430
假设我们进行第i个训练样本的

256
00:08:44.890 --> 00:08:48.010
正向传播

257
00:08:48.210 --> 00:08:50.180
和反向传播

258
00:08:51.030 --> 00:08:52.970
那么应该等于 y(i) 减去 a(4)1

259
00:08:53.250 --> 00:08:54.370
因此这实际是两者的偏差

260
00:08:54.560 --> 00:08:55.680
也就是 y 的实际值

261
00:08:56.000 --> 00:08:57.210
减去预测值

262
00:08:57.630 --> 00:08:58.020
得到的差值

263
00:08:58.530 --> 00:09:00.160
这样 我们就算出了

264
00:09:00.670 --> 00:09:01.880
δ(4)1 的值

265
00:09:03.510 --> 00:09:06.200
接下来我们要对这些值进行反向传播

266
00:09:06.910 --> 00:09:07.820
我稍后将详细解释

267
00:09:08.510 --> 00:09:10.810
计算出前一层的 δ 项的值

268
00:09:11.350 --> 00:09:12.450
那么这里我们计算出

269
00:09:12.560 --> 00:09:13.720
δ(3)1 和 δ(3)2

270
00:09:13.990 --> 00:09:15.210
然后同样的

271
00:09:15.600 --> 00:09:17.940
再进行下一层的反向传播

272
00:09:18.380 --> 00:09:19.340
这一次计算出

273
00:09:19.470 --> 00:09:21.960
δ(2)1

274
00:09:22.690 --> 00:09:23.800
以及 δ(2)2

275
00:09:25.190 --> 00:09:27.290
反向传播的计算

276
00:09:28.730 --> 00:09:30.050
和进行前向传播几乎相同

277
00:09:30.140 --> 00:09:32.870
唯一的区别就是方向相反

278
00:09:33.260 --> 00:09:33.890
我想表达的是

279
00:09:34.160 --> 00:09:35.300
我们来看我们是怎样得到

280
00:09:35.460 --> 00:09:37.370
δ(2)2 的值的

281
00:09:38.060 --> 00:09:39.280
我们要计算 δ(2)2

282
00:09:39.480 --> 00:09:42.330
与前向传播类似

283
00:09:42.600 --> 00:09:44.760
我要对一些权值进行标记

284
00:09:45.000 --> 00:09:47.620
那么这条权值 用桃红色表示的

285
00:09:47.890 --> 00:09:50.680
就是 θ(2)12

286
00:09:51.190 --> 00:09:54.190
然后这根箭头表示的权值

287
00:09:54.450 --> 00:09:55.970
我用红色来标记

288
00:09:56.280 --> 00:09:57.740
它代表的是

289
00:09:58.030 --> 00:09:59.760
θ(2)22

290
00:10:01.510 --> 00:10:03.410
所以

291
00:10:03.500 --> 00:10:05.450
我们来看

292
00:10:05.800 --> 00:10:07.540
δ(2)2是如何得到的

293
00:10:08.390 --> 00:10:09.690
实际上

294
00:10:09.800 --> 00:10:10.830
我们要做的是

295
00:10:10.970 --> 00:10:12.030
我们要用这个 δ 值

296
00:10:12.350 --> 00:10:14.340
和权值相乘

297
00:10:14.630 --> 00:10:16.770
然后加上

298
00:10:17.580 --> 00:10:18.660
这个 δ 值乘以权值的结果

299
00:10:18.930 --> 00:10:19.850
也就是说 它其实是

300
00:10:20.800 --> 00:10:22.880
这些δ值的加权和

301
00:10:23.280 --> 00:10:25.570
权值是这些对应边的强度

302
00:10:25.960 --> 00:10:27.270
让我把这些具体的值写出来

303
00:10:28.430 --> 00:10:29.550
δ(2)2 的值

304
00:10:30.270 --> 00:10:32.610
等于桃红色的这条权值

305
00:10:33.110 --> 00:10:34.660
θ(2)12

306
00:10:34.980 --> 00:10:38.850
乘以δ(3)1

307
00:10:38.990 --> 00:10:40.080
加上 下一个是用红色标记的权值

308
00:10:41.230 --> 00:10:43.530
θ(2)22

309
00:10:43.860 --> 00:10:46.230
乘上δ(3)2

310
00:10:46.700 --> 00:10:48.550
所以 简单地说

311
00:10:48.800 --> 00:10:51.340
就是红色的权值乘以它指向的值

312
00:10:51.570 --> 00:10:52.690
加上 桃红色的权值乘以它指向的值

313
00:10:53.540 --> 00:10:55.820
这样我们就得到了上一层的 δ 值

314
00:10:56.880 --> 00:10:59.490
再举一个例子

315
00:10:59.870 --> 00:11:00.750
我们来看这个 δ 值

316
00:11:01.320 --> 00:11:02.660
是怎么得到的呢？

317
00:11:02.890 --> 00:11:04.490
仍然是类似的过程

318
00:11:05.530 --> 00:11:07.000
如果这个权值

319
00:11:07.100 --> 00:11:08.310
用绿色表示的这根箭头

320
00:11:08.440 --> 00:11:09.860
假如这个权值

321
00:11:10.450 --> 00:11:12.990
是θ(3)12

322
00:11:13.920 --> 00:11:15.360
那么 δ(3)2

323
00:11:15.630 --> 00:11:17.010
将等于这条绿色的权值

324
00:11:17.910 --> 00:11:19.860
θ(3)12

325
00:11:20.800 --> 00:11:22.260
乘以 δ(4)1

326
00:11:22.930 --> 00:11:25.520
另外顺便提一下

327
00:11:25.610 --> 00:11:26.560
目前为止

328
00:11:26.670 --> 00:11:28.310
我写的 δ 值

329
00:11:28.660 --> 00:11:30.390
仅仅是隐藏层中的

330
00:11:30.560 --> 00:11:32.750
没有包括偏差单元+1

331
00:11:33.620 --> 00:11:34.610
包不包括偏差单元取决于你

332
00:11:35.030 --> 00:11:37.170
如何定义这个反向传播算法

333
00:11:37.330 --> 00:11:38.610
或者取决于你怎样实现这个算法

334
00:11:38.710 --> 00:11:40.510
你也可以

335
00:11:40.850 --> 00:11:42.390
对这些偏差单元

336
00:11:42.900 --> 00:11:43.950
计算 δ 的值

337
00:11:44.960 --> 00:11:46.230
这些偏差单元

338
00:11:46.620 --> 00:11:47.880
总是取为+1的值

339
00:11:47.990 --> 00:11:48.980
一直都这么取

340
00:11:49.220 --> 00:11:50.060
我们不能也没有必要

341
00:11:50.210 --> 00:11:51.960
更改偏差单元的值

342
00:11:52.340 --> 00:11:53.440
所以还是取决于你实现反向传播的方法

343
00:11:53.770 --> 00:11:54.960
通常说来 我在执行反向传播的时候

344
00:11:55.090 --> 00:11:56.180
我是算出了

345
00:11:56.340 --> 00:11:57.670
这些偏差单元的δ值

346
00:11:57.760 --> 00:11:58.900
但我通常忽略掉它们

347
00:11:58.990 --> 00:12:00.560
而不把它们代入计算

348
00:12:00.800 --> 00:12:02.130
因为它们其实

349
00:12:02.220 --> 00:12:04.130
并不是计算那些微分的必要部分

350
00:12:04.990 --> 00:12:06.720
好了 我希望这节课

351
00:12:06.990 --> 00:12:08.360
能给你一个 有关反向传播算法的实现过程

352
00:12:08.750 --> 00:12:10.380
更深刻的印象

353
00:12:12.480 --> 00:12:13.290
我知道可能这些过程

354
00:12:13.440 --> 00:12:14.670
还是看起来很神奇

355
00:12:14.760 --> 00:12:16.090
很“黑箱”

356
00:12:16.240 --> 00:12:17.560
不要紧 在后面的课程中

357
00:12:17.770 --> 00:12:19.880
在"putting it together"视频中

358
00:12:20.150 --> 00:12:22.650
我还会再介绍一点有关反向传播的内容

359
00:12:23.250 --> 00:12:24.360
但是很遗憾的是

360
00:12:24.450 --> 00:12:26.370
要想完全看清并且理解这个算法

361
00:12:26.510 --> 00:12:28.770
的确是很困难的

362
00:12:29.500 --> 00:12:30.790
但我想

363
00:12:30.990 --> 00:12:32.280
幸运的是 多年来

364
00:12:32.940 --> 00:12:33.930
很多人都能顺利地运用

365
00:12:34.420 --> 00:12:35.640
反向传播算法

366
00:12:35.730 --> 00:12:37.810
并且如果你执行一遍整个算法

367
00:12:37.990 --> 00:12:40.090
你就能掌握这种很强大的机器学习算法

368
00:12:40.340 --> 00:12:41.400
尽管它内部的工作原理

369
00:12:41.900 --> 00:12:43.190
的确显得很难观察 【教育无边界字幕组】翻译:所罗门捷列夫 校对:Roy薛 审核:柳桦