WEBVTT

1
00:00:00.540 --> 00:00:01.820
在前面的视频中

2
00:00:01.950 --> 00:00:03.220
我们总结了

3
00:00:03.270 --> 00:00:04.620
在神经网络的实现和训练中

4
00:00:04.820 --> 00:00:07.170
所有需要的知识

5
00:00:07.940 --> 00:00:09.060
这是最后一个

6
00:00:09.120 --> 00:00:09.980
我想要分享给你们的内容

7
00:00:10.200 --> 00:00:11.570
这就是随机初始化的思想

8
00:00:13.220 --> 00:00:14.360
当你运行一个算法

9
00:00:14.510 --> 00:00:15.990
例如梯度下降算法

10
00:00:16.280 --> 00:00:17.810
或者其他高级优化算法时

11
00:00:17.940 --> 00:00:20.770
我们需要给变量 θ 一些初始值

12
00:00:21.610 --> 00:00:22.990
所以对于那些高级的优化算法

13
00:00:23.570 --> 00:00:24.620
假设

14
00:00:24.780 --> 00:00:26.090
我们给变量 θ

15
00:00:26.700 --> 00:00:27.640
传递一些初始值

16
00:00:29.010 --> 00:00:30.680
现在让我们考虑梯度下降

17
00:00:31.320 --> 00:00:34.090
同样 我们需要把 θ 初始化成一些值

18
00:00:34.580 --> 00:00:36.030
接下来使用梯度下降方法

19
00:00:36.680 --> 00:00:38.830
慢慢地执行这些步骤使其下降

20
00:00:38.910 --> 00:00:40.920
使 θ 的函数 J 下降到最小

21
00:00:41.990 --> 00:00:43.960
那么 θ 的初始值该设置为多少呢？

22
00:00:44.240 --> 00:00:47.000
是否可以

23
00:00:47.520 --> 00:00:48.930
将 θ 的初始值设为

24
00:00:49.250 --> 00:00:50.450
全部是0的向量

25
00:00:51.870 --> 00:00:54.800
虽然说在逻辑回归时

26
00:00:55.630 --> 00:00:56.690
初始化所有变量为0

27
00:00:56.760 --> 00:00:57.970
是可行的的

28
00:00:58.310 --> 00:01:00.290
但在训练神经网络时这样做是不可行的

29
00:01:01.410 --> 00:01:03.150
以训练这个神经网络为例

30
00:01:03.650 --> 00:01:06.430
照之前所说将所有变量初始化为0

31
00:01:07.970 --> 00:01:09.210
如果是这样的话

32
00:01:09.780 --> 00:01:10.920
具体来说就是

33
00:01:11.160 --> 00:01:13.870
当初始化这条蓝色权重

34
00:01:15.390 --> 00:01:16.540
使这条被涂为蓝色的权重等于那条蓝色的权重

35
00:01:17.510 --> 00:01:17.510
他们都是0

36
00:01:18.580 --> 00:01:19.880
这条被涂上红色的权重

37
00:01:20.330 --> 00:01:21.940
同样等于

38
00:01:22.550 --> 00:01:23.040
被涂上红色的这条权重

39
00:01:23.790 --> 00:01:25.280
同样这个权重

40
00:01:25.620 --> 00:01:26.500
这个被涂成绿色的权重也一样

41
00:01:26.680 --> 00:01:28.940
等于那条绿色的权重

42
00:01:30.030 --> 00:01:32.820
那么这就意味着这两个隐藏单元 a1 a2

43
00:01:32.950 --> 00:01:35.940
是两个相同的关于

44
00:01:36.660 --> 00:01:36.810
输入的函数

45
00:01:37.810 --> 00:01:38.900
这样一来

46
00:01:39.500 --> 00:01:40.870
对每个样本进行训练

47
00:01:41.430 --> 00:01:43.640
最后a(2)1与a(2)2结果必然相等

48
00:01:46.950 --> 00:01:48.700
更多的原因

49
00:01:48.960 --> 00:01:50.050
我就不详细讲述了

50
00:01:50.310 --> 00:01:51.420
而由于

51
00:01:51.580 --> 00:01:52.990
这些权重相同

52
00:01:53.080 --> 00:01:54.630
同样可以证明

53
00:01:54.710 --> 00:01:56.560
这些 δ 值也相同

54
00:01:56.790 --> 00:01:57.790
具体地说

55
00:01:57.970 --> 00:02:00.070
δ(2)1=δ(2)2

56
00:02:00.760 --> 00:02:02.900
δ(2)1=δ(2)2

57
00:02:06.120 --> 00:02:07.150
同时 如果你更深入地挖掘一下

58
00:02:07.230 --> 00:02:08.480
你不难得出

59
00:02:08.760 --> 00:02:09.990
这些变量对参数的偏导数

60
00:02:11.560 --> 00:02:14.080
满足以下条件

61
00:02:15.120 --> 00:02:16.710
也就是 代价函数的

62
00:02:17.550 --> 00:02:19.260
偏导数

63
00:02:19.580 --> 00:02:21.020
关于

64
00:02:21.800 --> 00:02:23.680
我用这两条

65
00:02:23.900 --> 00:02:25.320
蓝色的权重为例

66
00:02:26.190 --> 00:02:27.290
你不难发现

67
00:02:27.680 --> 00:02:30.340
这两个偏导数互为相等

68
00:02:31.970 --> 00:02:33.180
这也就意味着

69
00:02:33.320 --> 00:02:35.820
一旦更新梯度下降方法

70
00:02:36.690 --> 00:02:38.200
第一个蓝色权重也会更新

71
00:02:38.470 --> 00:02:40.800
等于学习率乘以这个式子

72
00:02:41.580 --> 00:02:42.500
第二条蓝色权重更新为

73
00:02:42.920 --> 00:02:44.620
学习率乘上这个式子

74
00:02:44.820 --> 00:02:45.870
但是 这就意味着

75
00:02:45.980 --> 00:02:47.090
一旦更新梯度下降

76
00:02:47.420 --> 00:02:49.330
这两条

77
00:02:49.680 --> 00:02:50.710
蓝色权重的值

78
00:02:51.430 --> 00:02:53.050
在最后将

79
00:02:53.240 --> 00:02:54.960
将互为相等

80
00:02:55.190 --> 00:02:56.210
因此 即使权重现在不都为0

81
00:02:56.750 --> 00:02:57.720
但参数的值

82
00:02:58.550 --> 00:02:59.520
最后也互为相等

83
00:03:00.360 --> 00:03:02.790
同样地 即使更新一个梯度下降

84
00:03:03.690 --> 00:03:05.740
这条红色的权重也会等于这条红色的权重

85
00:03:06.170 --> 00:03:07.200
也许会有些非0的值

86
00:03:07.640 --> 00:03:09.450
但两条红色的值会互为相等

87
00:03:10.240 --> 00:03:11.760
同样两条绿色的权重

88
00:03:12.060 --> 00:03:13.720
开始它们有不同的值

89
00:03:13.860 --> 00:03:16.350
最后这两个权重也会互为相等

90
00:03:17.590 --> 00:03:19.020
所以每次更新后

91
00:03:19.740 --> 00:03:20.890
两个隐藏单元的输入对应的参数

92
00:03:21.060 --> 00:03:22.870
将是相同的

93
00:03:23.700 --> 00:03:24.490
这只是说

94
00:03:24.710 --> 00:03:25.590
两条绿色的权重将一直相同

95
00:03:25.640 --> 00:03:26.310
两条红色的权重将一直相同

96
00:03:26.550 --> 00:03:27.750
两条蓝色的权重

97
00:03:28.010 --> 00:03:30.000
仍然相同

98
00:03:30.160 --> 00:03:31.590
这就意味着

99
00:03:31.770 --> 00:03:33.070
即使经过一次循环后

100
00:03:33.460 --> 00:03:34.860
梯度下降的循环后

101
00:03:35.600 --> 00:03:37.250
你们会发现两个隐藏单元

102
00:03:37.800 --> 00:03:40.380
仍然是两个完全相同的输入函数

103
00:03:40.830 --> 00:03:43.040
因此 a(1)2 仍然等于 a(2)2

104
00:03:43.510 --> 00:03:45.200
回到这里

105
00:03:45.930 --> 00:03:47.380
一直持续运行梯度下降

106
00:03:48.390 --> 00:03:50.940
这两条蓝色的权重仍然相同

107
00:03:51.190 --> 00:03:52.920
两条红色的权重 两条绿色的权重

108
00:03:53.060 --> 00:03:54.990
也是同样的情况

109
00:03:55.160 --> 00:03:56.860
这也就意味着

110
00:03:57.130 --> 00:03:58.260
这个神经网络

111
00:03:58.470 --> 00:03:59.980
的确不能计算更有价值的东西

112
00:04:00.700 --> 00:04:01.910
想象一下

113
00:04:02.240 --> 00:04:03.670
不止有两个隐藏单元

114
00:04:04.010 --> 00:04:05.470
而是

115
00:04:05.640 --> 00:04:07.100
有很多很多的隐藏单元

116
00:04:08.080 --> 00:04:09.160
这就是说

117
00:04:09.430 --> 00:04:10.680
所有的隐藏单元

118
00:04:10.740 --> 00:04:12.320
都在计算相同的特征

119
00:04:12.540 --> 00:04:16.300
所有的隐藏单元都通过完全相同的输入函数计算出来

120
00:04:17.030 --> 00:04:18.980
这是完全多余的表达

121
00:04:20.140 --> 00:04:21.010
因为 这意味着

122
00:04:21.110 --> 00:04:24.160
最后的逻辑回归单元只会得到一种特征

123
00:04:24.730 --> 00:04:25.460
因为所有的逻辑回归单元都一样

124
00:04:26.330 --> 00:04:28.690
这样便阻止了神经网络学习出更有价值的信息

125
00:04:31.600 --> 00:04:32.830
为了解决这个问题

126
00:04:32.960 --> 00:04:34.050
神经网络变量

127
00:04:34.590 --> 00:04:35.680
初始化的方式

128
00:04:36.050 --> 00:04:37.660
采用随机初始化

129
00:04:41.820 --> 00:04:43.130
具体地说

130
00:04:43.250 --> 00:04:44.470
在上一张幻灯片中看到的

131
00:04:44.760 --> 00:04:46.240
所有权重相同的问题 

132
00:04:46.640 --> 00:04:49.040
有时被我们也称为对称权重

133
00:04:49.810 --> 00:04:51.470
所以随机初始化

134
00:04:52.590 --> 00:04:54.240
解决的就是如何打破这种对称性

135
00:04:55.520 --> 00:04:56.480
所以 我们需要做的是

136
00:04:56.680 --> 00:04:58.200
对 θ 的每个值

137
00:04:58.310 --> 00:04:59.460
进行初始化

138
00:04:59.830 --> 00:05:01.300
范围在 -ɛ 到 +ɛ 之间

139
00:05:02.080 --> 00:05:03.200
这个方括号意味着

140
00:05:03.310 --> 00:05:05.350
-ɛ 到 +ɛ 之间

141
00:05:06.330 --> 00:05:07.430
因此

142
00:05:07.540 --> 00:05:08.660
变量的权重通常初始化为

143
00:05:08.710 --> 00:05:11.470
-ɛ 到 +ɛ 之间的任意一个数

144
00:05:12.300 --> 00:05:13.330
我在 Octave 里编写了这样的代码

145
00:05:13.420 --> 00:05:16.770
我之前讲过的 Theta1 等于这个等式

146
00:05:17.550 --> 00:05:19.620
所以 这个 10×11 的随机矩阵 

147
00:05:19.910 --> 00:05:21.060
这个 rand 就是用来

148
00:05:21.640 --> 00:05:23.620
得出一个任意的 10×11 维矩阵

149
00:05:24.670 --> 00:05:26.640
矩阵中的所有值

150
00:05:27.070 --> 00:05:30.380
都介于0到1之间

151
00:05:30.580 --> 00:05:31.350
所以

152
00:05:31.520 --> 00:05:32.700
这些实数

153
00:05:32.870 --> 00:05:34.860
取0到1之间的连续值

154
00:05:35.450 --> 00:05:36.290
因此

155
00:05:36.320 --> 00:05:37.440
如果取0到1之间的一个数

156
00:05:37.550 --> 00:05:38.310
和

157
00:05:38.590 --> 00:05:39.550
2ε 相乘

158
00:05:39.600 --> 00:05:41.050
再减去 ε 

159
00:05:41.160 --> 00:05:42.270
然后得到

160
00:05:42.690 --> 00:05:44.160
一个在 -ε 到 +ε 的数

161
00:05:45.640 --> 00:05:46.970
顺便说一句

162
00:05:47.230 --> 00:05:48.410
这里的这个 ε 

163
00:05:48.730 --> 00:05:49.860
在进行梯度检查中用的

164
00:05:50.070 --> 00:05:51.710
不是一回事

165
00:05:52.590 --> 00:05:54.070
因此在进行数值梯度检查时

166
00:05:54.850 --> 00:05:57.060
会加一些 ε 值给 θ 

167
00:05:57.430 --> 00:05:59.560
这些值与这里的ε 没有关系

168
00:05:59.780 --> 00:06:00.590
这就是为什么我要在这里用 INIT_EPSILON 表示

169
00:06:00.990 --> 00:06:02.200
仅仅是为了区分

170
00:06:02.480 --> 00:06:04.970
在梯度检查中使用的 EPSILON 值

171
00:06:06.490 --> 00:06:07.590
当然 类似的

172
00:06:07.690 --> 00:06:09.620
如果想要初始化θ2

173
00:06:09.640 --> 00:06:10.820
为任意一个1×11的矩阵

174
00:06:10.920 --> 00:06:13.430
可以使用这里的这段代码

175
00:06:15.910 --> 00:06:17.460
总结来说

176
00:06:17.660 --> 00:06:18.910
为了训练神经网络

177
00:06:19.060 --> 00:06:20.850
应该对权重进行随机初始化

178
00:06:20.930 --> 00:06:21.810
初始化为

179
00:06:22.120 --> 00:06:23.370
 --ε到+ε间

180
00:06:23.740 --> 00:06:24.740
接近于0的小数

181
00:06:25.160 --> 00:06:27.150
然后进行反向传播

182
00:06:27.620 --> 00:06:29.330
执行梯度检查

183
00:06:30.220 --> 00:06:31.300
使用梯度下降

184
00:06:31.660 --> 00:06:32.620
或者

185
00:06:32.880 --> 00:06:34.860
使用高级的优化算法

186
00:06:35.100 --> 00:06:36.250
试着使代价函数 J 

187
00:06:36.790 --> 00:06:37.860
达到最小

188
00:06:38.050 --> 00:06:39.610
从某个随机选取的

189
00:06:39.890 --> 00:06:41.900
参数 θ 开始

190
00:06:42.970 --> 00:06:45.440
通过打破对称性的过程

191
00:06:46.000 --> 00:06:47.110
我们希望梯度下降

192
00:06:47.580 --> 00:06:48.820
或者其他高级优化算法

193
00:06:48.980 --> 00:06:50.710
可以找到θ的最优值 【教育无边界字幕组】翻译：jvnikita 校对/审核：所罗门捷列夫