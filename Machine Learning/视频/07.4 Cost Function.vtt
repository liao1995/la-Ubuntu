WEBVTT

1
00:00:00.160 --> 00:00:01.704
在这段视频中 我们要讲

2
00:00:01.704 --> 00:00:04.010
如何拟合逻辑回归

3
00:00:04.040 --> 00:00:05.869
模型的参数θ

4
00:00:05.880 --> 00:00:06.982
具体来说 我要定义

5
00:00:07.020 --> 00:00:10.386
用来拟合参数的

6
00:00:10.400 --> 00:00:14.470
优化目标或者叫代价函数

7
00:00:15.390 --> 00:00:17.370
这便是监督学习问题中的

8
00:00:17.370 --> 00:00:19.892
逻辑回归模型的拟合问题

9
00:00:19.960 --> 00:00:22.210
我们有一个训练集

10
00:00:22.210 --> 00:00:24.964
里面有m个训练样本

11
00:00:24.964 --> 00:00:26.577
像以前一样

12
00:00:26.577 --> 00:00:28.130
我们的每个样本

13
00:00:28.150 --> 00:00:32.830
用n+1维的特征向量表示

14
00:00:32.830 --> 00:00:35.133
同样和以前一样

15
00:00:35.180 --> 00:00:36.498
x0 = 1

16
00:00:36.498 --> 00:00:38.315
第一个特征变量

17
00:00:38.315 --> 00:00:39.951
 或者说第0个特征变量 一直是1

18
00:00:39.970 --> 00:00:41.203
而且因为这是一个分类问题

19
00:00:41.203 --> 00:00:43.335
我们的训练集

20
00:00:43.350 --> 00:00:44.999
具有这样的特征

21
00:00:45.010 --> 00:00:48.422
所有的y 不是0就是1

22
00:00:48.422 --> 00:00:50.576
这是一个假设函数

23
00:00:50.576 --> 00:00:52.007
它的参数

24
00:00:52.007 --> 00:00:54.460
是这里的这个θ

25
00:00:54.490 --> 00:00:55.572
我要说的问题是

26
00:00:55.610 --> 00:00:57.339
对于这个给定的训练集

27
00:00:57.340 --> 00:00:58.846
我们如何选择

28
00:00:58.880 --> 00:01:02.482
或者说如何拟合参数θ

29
00:01:02.510 --> 00:01:04.125
以前我们推导线性回归时

30
00:01:04.125 --> 00:01:08.463
使用了这个代价函数

31
00:01:08.480 --> 00:01:10.868
我把这个写成稍微有点儿不同的形式

32
00:01:10.900 --> 00:01:12.663
不写原先的1/2m

33
00:01:12.670 --> 00:01:16.440
我把1/2放到求和符号里面了

34
00:01:16.440 --> 00:01:17.440
现在我想用

35
00:01:17.440 --> 00:01:19.132
另一种方法

36
00:01:19.140 --> 00:01:20.663
来写代价函数

37
00:01:20.700 --> 00:01:22.009
去掉这个平方项

38
00:01:22.030 --> 00:01:23.920
把这里写成

39
00:01:23.920 --> 00:01:27.100
这样的形式

40
00:01:28.310 --> 00:01:31.476
（具体公式请看屏幕）

41
00:01:31.500 --> 00:01:33.605
（具体公式请看屏幕）

42
00:01:33.605 --> 00:01:37.176
定义这个代价函数Cost函数

43
00:01:37.210 --> 00:01:39.727
等于这个

44
00:01:39.740 --> 00:01:42.641
等于这个1/2的平方根误差

45
00:01:42.670 --> 00:01:43.800
因此现在

46
00:01:43.800 --> 00:01:46.018
我们能更清楚的看到

47
00:01:46.018 --> 00:01:48.145
代价函数是这个Cost函数

48
00:01:48.145 --> 00:01:49.740
在训练集范围上的求和

49
00:01:49.740 --> 00:01:51.427
或者说是1/m倍的

50
00:01:51.427 --> 00:01:56.046
这个代价项在训练集范围上的求和

51
00:01:56.050 --> 00:01:58.065
然后稍微简化一下这个式子

52
00:01:58.065 --> 00:01:59.470
去掉这些上标

53
00:01:59.490 --> 00:02:02.587
会显得方便一些

54
00:02:02.610 --> 00:02:04.408
所以直接定义

55
00:02:04.408 --> 00:02:05.527
代价值(h(X), Y)

56
00:02:05.527 --> 00:02:06.618
等于1/2倍的

57
00:02:06.618 --> 00:02:08.925
这个平方根误差

58
00:02:08.925 --> 00:02:10.336
对这个代价项的理解是这样的

59
00:02:10.360 --> 00:02:11.876
这是我所期望的

60
00:02:11.890 --> 00:02:13.447
我的学习算法

61
00:02:13.460 --> 00:02:15.110
如果想要达到这个值

62
00:02:15.110 --> 00:02:16.701
也就是这个假设h(x)

63
00:02:16.750 --> 00:02:18.737
所需要付出的代价

64
00:02:18.737 --> 00:02:19.912
这个希望的预测值是h(x)

65
00:02:19.912 --> 00:02:21.258
而实际值则是y

66
00:02:21.310 --> 00:02:24.035
干脆

67
00:02:24.050 --> 00:02:27.836
全部去掉那些上标好了

68
00:02:27.840 --> 00:02:29.756
显然 在线性回归中

69
00:02:29.756 --> 00:02:31.537
代价值会被定义为这个

70
00:02:31.537 --> 00:02:32.757
这个代价值是

71
00:02:32.757 --> 00:02:34.535
1/2乘以

72
00:02:34.540 --> 00:02:36.232
预测值h和

73
00:02:36.232 --> 00:02:37.663
实际值观测的结果y

74
00:02:37.670 --> 00:02:38.943
的差的平方

75
00:02:38.943 --> 00:02:41.103
这个代价值可以

76
00:02:41.103 --> 00:02:42.848
很好地用在线性回归里

77
00:02:42.848 --> 00:02:47.418
但是我们现在要用在逻辑回归里

78
00:02:47.430 --> 00:02:49.146
如果我们可以最小化

79
00:02:49.150 --> 00:02:51.992
代价函数J里面的这个代价值

80
00:02:52.020 --> 00:02:53.817
它会工作得很好

81
00:02:53.817 --> 00:02:55.476
但实际上

82
00:02:55.480 --> 00:02:57.640
如果我们使用这个代价值

83
00:02:57.640 --> 00:03:01.807
 它会变成参数θ的非凸函数

84
00:03:01.820 --> 00:03:03.968
我说的非凸函数是这个意思

85
00:03:03.990 --> 00:03:05.313
对于这样一个代价函数J(θ)

86
00:03:05.313 --> 00:03:08.118
对于逻辑回归来说

87
00:03:08.140 --> 00:03:12.113
这里的h函数

88
00:03:12.113 --> 00:03:13.495
是非线性的 对吧？

89
00:03:13.500 --> 00:03:14.538
它是等于 1 除以

90
00:03:14.538 --> 00:03:16.384
1+e的-θ转置乘以X次方

91
00:03:16.384 --> 00:03:19.591
所以它是一个很复杂的非线性函数

92
00:03:19.591 --> 00:03:21.108
如果对它取Sigmoid函数

93
00:03:21.130 --> 00:03:22.104
然后把它放到这里

94
00:03:22.104 --> 00:03:23.239
然后求它的代价值

95
00:03:23.300 --> 00:03:25.016
再把它放到这里

96
00:03:25.020 --> 00:03:26.746
然后再画出

97
00:03:26.746 --> 00:03:28.200
J(θ)长什么模样

98
00:03:28.210 --> 00:03:29.650
你会发现

99
00:03:29.650 --> 00:03:33.493
J(θ)可能是一个这样的函数

100
00:03:33.500 --> 00:03:35.958
有很多局部最优值

101
00:03:35.958 --> 00:03:37.321
称呼它的正式术语是

102
00:03:37.340 --> 00:03:39.488
这是一个非凸函数

103
00:03:39.500 --> 00:03:40.644
你大概可以发现

104
00:03:40.644 --> 00:03:41.880
如果你把梯度下降法

105
00:03:41.880 --> 00:03:43.192
用在一个这样的函数上

106
00:03:43.192 --> 00:03:45.160
不能保证它会

107
00:03:45.170 --> 00:03:47.747
收敛到全局最小值

108
00:03:47.747 --> 00:03:48.867
相应地

109
00:03:48.870 --> 00:03:50.350
我们希望

110
00:03:50.350 --> 00:03:52.100
我们的代价函数J(θ)

111
00:03:52.100 --> 00:03:53.599
是一个凸函数

112
00:03:53.599 --> 00:03:55.250
是一个单弓形函数

113
00:03:55.250 --> 00:03:56.675
大概是这样

114
00:03:56.675 --> 00:03:58.543
所以如果对它使用梯度下降法

115
00:03:58.543 --> 00:04:01.147
我们可以保证梯度下降法

116
00:04:01.170 --> 00:04:04.917
会收敛到该函数的全局最小值

117
00:04:04.917 --> 00:04:07.020
但使用这个

118
00:04:07.020 --> 00:04:08.460
平方代价函数的问题是

119
00:04:08.520 --> 00:04:10.400
因为中间的这个

120
00:04:10.400 --> 00:04:12.371
非常非线性的

121
00:04:12.371 --> 00:04:14.107
sigmoid函数的出现

122
00:04:14.107 --> 00:04:15.987
导致J(θ)成为

123
00:04:15.987 --> 00:04:17.962
一个非凸函数

124
00:04:17.962 --> 00:04:21.294
如果你要用平方函数定义它的话

125
00:04:21.294 --> 00:04:22.313
所以我们想做的是

126
00:04:22.320 --> 00:04:23.822
另外找一个

127
00:04:23.822 --> 00:04:25.576
不同的代价函数

128
00:04:25.576 --> 00:04:28.063
它是凸函数

129
00:04:28.063 --> 00:04:29.257
使得我们可以使用很好的算法

130
00:04:29.280 --> 00:04:30.919
如梯度下降法

131
00:04:30.940 --> 00:04:33.683
而且能保证找到全局最小值

132
00:04:33.683 --> 00:04:37.295
这个代价函数便是我们要用在逻辑回归上的

133
00:04:37.295 --> 00:04:39.313
我们认为

134
00:04:39.320 --> 00:04:40.710
这个算法要付的代价或者惩罚

135
00:04:40.710 --> 00:04:42.924
如果输出值是h(x)

136
00:04:42.924 --> 00:04:44.596
或者换句话说

137
00:04:44.620 --> 00:04:46.722
假如说预测值h(x)

138
00:04:46.722 --> 00:04:48.670
是一个数 比如0.7

139
00:04:48.670 --> 00:04:50.780
而实际上

140
00:04:50.780 --> 00:04:52.032
真实的标签值是y

141
00:04:52.032 --> 00:04:54.087
那么代价值将等于

142
00:04:54.090 --> 00:04:56.061
-log(h(X))

143
00:04:56.100 --> 00:04:57.861
当y=1时

144
00:04:57.861 --> 00:04:59.447
以及-log(1-h(X))

145
00:04:59.460 --> 00:05:02.010
当y=0时

146
00:05:02.020 --> 00:05:04.205
这看起来是个非常复杂的函数

147
00:05:04.230 --> 00:05:05.773
但是让我们画出这个函数

148
00:05:05.773 --> 00:05:08.147
可以直观地感受一下它在做什么

149
00:05:08.160 --> 00:05:11.054
我们从y=1这个情况开始

150
00:05:11.070 --> 00:05:12.461
如果y等于1

151
00:05:12.461 --> 00:05:14.958
那么这个代价函数

152
00:05:14.958 --> 00:05:18.240
是-log(h(X))

153
00:05:18.240 --> 00:05:19.601
如果我们画出它

154
00:05:19.601 --> 00:05:21.564
我们将h(X)

155
00:05:21.580 --> 00:05:22.961
画在横坐标上

156
00:05:22.961 --> 00:05:24.722
我们知道假设函数

157
00:05:24.730 --> 00:05:26.611
的输出值

158
00:05:26.630 --> 00:05:28.465
是在0和1之间的

159
00:05:28.465 --> 00:05:28.465
对吧？

160
00:05:28.490 --> 00:05:30.514
所以h(X)的值

161
00:05:30.530 --> 00:05:31.940
在0和1之间变化

162
00:05:31.940 --> 00:05:35.469
如果你画出这个代价函数的样子

163
00:05:35.470 --> 00:05:37.981
你会发现它看起来是这样的

164
00:05:37.981 --> 00:05:39.044
理解这个函数为什么是这样的

165
00:05:39.044 --> 00:05:41.363
一个方式是

166
00:05:41.440 --> 00:05:44.988
如果你画出log(z)

167
00:05:45.000 --> 00:05:47.656
z在横轴上

168
00:05:47.656 --> 00:05:48.794
它看起来会是这样

169
00:05:48.794 --> 00:05:50.369
它趋于负无穷

170
00:05:50.369 --> 00:05:53.700
这是对数函数的样子

171
00:05:53.700 --> 00:05:55.963
所以这里是0 这里是1

172
00:05:55.980 --> 00:05:57.560
显然 这里的Z

173
00:05:57.560 --> 00:05:59.653
就是代表h(x)的角色

174
00:05:59.653 --> 00:06:02.030
因此

175
00:06:02.030 --> 00:06:06.329
-log(Z)看起来这样

176
00:06:06.330 --> 00:06:08.098
就是翻转一下符号

177
00:06:08.100 --> 00:06:09.822
-log(Z)

178
00:06:09.822 --> 00:06:11.013
我们所感兴趣的是

179
00:06:11.020 --> 00:06:12.580
函数在0到1

180
00:06:12.610 --> 00:06:14.014
之间的这个区间

181
00:06:14.014 --> 00:06:15.924
所以 忽略那些

182
00:06:15.924 --> 00:06:17.962
所以只剩下

183
00:06:17.980 --> 00:06:21.555
曲线的这部分

184
00:06:21.630 --> 00:06:23.200
这就是左边这条曲线的样子

185
00:06:23.200 --> 00:06:25.472
现在这个代价函数

186
00:06:25.500 --> 00:06:29.666
有一些有趣而且很好的性质

187
00:06:29.690 --> 00:06:32.103
首先 你注意到

188
00:06:32.103 --> 00:06:35.003
如果y=1而且h(X)=1

189
00:06:35.010 --> 00:06:37.367
也就是说

190
00:06:37.410 --> 00:06:39.000
如果假设函数

191
00:06:39.000 --> 00:06:40.261
刚好预测值是1

192
00:06:40.261 --> 00:06:42.744
而且y刚好等于我预测的

193
00:06:42.744 --> 00:06:44.432
那么这个代价值等于0

194
00:06:44.432 --> 00:06:44.432
对吧？

195
00:06:44.432 --> 00:06:47.475
这对应于… 这个曲线并不是平的

196
00:06:47.475 --> 00:06:49.866
曲线还在继续走

197
00:06:49.880 --> 00:06:51.006
首先 注意到如果h(x)=1

198
00:06:51.006 --> 00:06:53.056
如果假设函数

199
00:06:53.056 --> 00:06:55.113
预测Y=1

200
00:06:55.113 --> 00:06:56.342
并且如果y确实等于1

201
00:06:56.342 --> 00:06:58.502
那么代价值等于0

202
00:06:58.530 --> 00:07:00.975
这对应于下面这个点

203
00:07:00.975 --> 00:07:00.975
对吧？

204
00:07:01.030 --> 00:07:02.332
如果h(X)=1

205
00:07:02.332 --> 00:07:04.068
这里我们只需要考虑

206
00:07:04.068 --> 00:07:06.273
y=1的情况

207
00:07:06.273 --> 00:07:08.366
如果h(x)等于1

208
00:07:08.366 --> 00:07:11.063
那么代价值等于0

209
00:07:11.063 --> 00:07:13.082
这是我们所希望的

210
00:07:13.082 --> 00:07:13.968
因为如果我们

211
00:07:13.968 --> 00:07:17.673
正确预测了输出值y 那么代价值是0

212
00:07:17.673 --> 00:07:21.466
但是现在 同样注意到

213
00:07:21.470 --> 00:07:23.456
h(x)趋于0时

214
00:07:23.456 --> 00:07:25.037
所以 那是h

215
00:07:25.037 --> 00:07:26.909
当假设函数的输出趋于0时

216
00:07:26.909 --> 00:07:30.163
代价值激增 并且趋于无穷

217
00:07:30.163 --> 00:07:31.513
我们这样描述

218
00:07:31.513 --> 00:07:34.271
体现出了这样一种直观的感觉

219
00:07:34.310 --> 00:07:36.890
那就是如果假设函数输出0

220
00:07:36.890 --> 00:07:38.574
相当于说

221
00:07:38.574 --> 00:07:39.960
我们的假设函数说

222
00:07:39.960 --> 00:07:41.541
Y=1的概率等于0

223
00:07:41.541 --> 00:07:42.516
这类似于

224
00:07:42.520 --> 00:07:44.010
我们对病人说

225
00:07:44.020 --> 00:07:45.594
你有一个恶性肿瘤的概率

226
00:07:45.610 --> 00:07:47.337
也就是说

227
00:07:47.337 --> 00:07:49.807
y=1的概率是0

228
00:07:49.807 --> 00:07:52.154
就是说你的肿瘤

229
00:07:52.160 --> 00:07:55.130
完全不可能是恶性的

230
00:07:55.150 --> 00:07:56.776
然而结果是

231
00:07:56.776 --> 00:08:00.111
病人的肿瘤确实是恶性的

232
00:08:00.111 --> 00:08:01.879
所以如果y=1

233
00:08:01.880 --> 00:08:03.291
即使我们告诉他们

234
00:08:03.300 --> 00:08:05.375
它发生的概率是0

235
00:08:05.390 --> 00:08:08.716
它完全不可能是恶性的

236
00:08:08.716 --> 00:08:09.759
如果我们告诉他们这个

237
00:08:09.760 --> 00:08:11.186
和我们的确信程度

238
00:08:11.240 --> 00:08:13.018
并且最后我们是错的

239
00:08:13.018 --> 00:08:14.688
那么我们用非常非常大的代价值

240
00:08:14.690 --> 00:08:16.122
惩罚这个学习算法

241
00:08:16.122 --> 00:08:17.963
它是被这样体现出来

242
00:08:17.963 --> 00:08:20.474
这个代价值趋于无穷

243
00:08:20.474 --> 00:08:21.900
如果y=1

244
00:08:21.900 --> 00:08:24.334
而h(x)趋于0

245
00:08:24.334 --> 00:08:26.725
这是y=1时的情况

246
00:08:26.725 --> 00:08:28.875
我们再来看看

247
00:08:28.875 --> 00:08:32.371
y=0时 代价值函数是什么样

248
00:08:32.410 --> 00:08:35.710
如果y=0

249
00:08:35.720 --> 00:08:39.121
那么代价值是这个表达式

250
00:08:39.121 --> 00:08:40.403
如果画出函数

251
00:08:40.403 --> 00:08:42.751
-log(1-z)

252
00:08:42.780 --> 00:08:45.839
那么你得到的

253
00:08:45.839 --> 00:08:49.245
代价函数实际上是这样

254
00:08:49.245 --> 00:08:50.256
它从0到1

255
00:08:50.270 --> 00:08:53.263
差不多这样

256
00:08:53.280 --> 00:08:54.611
如果你画出

257
00:08:54.611 --> 00:08:55.872
y=0情况下的

258
00:08:55.872 --> 00:08:57.823
代价函数

259
00:08:57.823 --> 00:09:00.763
你会发现大概是这样

260
00:09:00.763 --> 00:09:02.404
它现在所做的是

261
00:09:02.404 --> 00:09:04.937
在h(X)趋于1时激增

262
00:09:04.937 --> 00:09:08.273
趋于正无穷

263
00:09:08.290 --> 00:09:09.880
因为它是说

264
00:09:09.900 --> 00:09:11.199
如果最后发现

265
00:09:11.200 --> 00:09:12.168
y等于0

266
00:09:12.168 --> 00:09:13.966
而我们却几乎

267
00:09:13.966 --> 00:09:15.286
非常肯定地预测

268
00:09:15.320 --> 00:09:17.281
y=1的概率是1

269
00:09:17.281 --> 00:09:21.569
那么我们最后就要付出非常大的代价值

270
00:09:21.569 --> 00:09:23.143
（以下这一段和前面重复了）让我们画出y=0时的

271
00:09:23.143 --> 00:09:25.063
代价函数

272
00:09:25.063 --> 00:09:29.702
所以如果y=0 这就是我们的代价值函数

273
00:09:29.702 --> 00:09:31.914
如果你看着这个表达式

274
00:09:31.914 --> 00:09:33.726
然后你画出

275
00:09:33.726 --> 00:09:36.221
-log(1-Z)

276
00:09:36.221 --> 00:09:37.428
如果你清楚它是什么样的

277
00:09:37.428 --> 00:09:40.071
你会得到这样一个图形

278
00:09:40.071 --> 00:09:41.669
这样随着

279
00:09:41.680 --> 00:09:43.610
横轴上的z

280
00:09:43.610 --> 00:09:45.850
从0到1

281
00:09:45.850 --> 00:09:47.221
如果你画出

282
00:09:47.221 --> 00:09:48.397
y=0时的

283
00:09:48.397 --> 00:09:49.614
代价函数

284
00:09:49.614 --> 00:09:51.186
你会发现

285
00:09:51.186 --> 00:09:55.109
代价函数是这样的

286
00:09:55.109 --> 00:09:56.743
它所做的是

287
00:09:56.743 --> 00:09:58.650
代价函数会在这里激增

288
00:09:58.660 --> 00:09:59.530
趋于正无穷

289
00:09:59.560 --> 00:10:01.448
随着h(X)的增大

290
00:10:01.448 --> 00:10:03.707
而趋近于1

291
00:10:03.710 --> 00:10:05.443
这体现了这样一个直观的感觉

292
00:10:05.443 --> 00:10:07.159
如果假设函数预测

293
00:10:07.180 --> 00:10:08.847
h(X)=1

294
00:10:08.850 --> 00:10:10.406
并且非常确定

295
00:10:10.406 --> 00:10:12.121
比如这样的概率是1

296
00:10:12.121 --> 00:10:14.283
认为y肯定是1

297
00:10:14.283 --> 00:10:15.563
但是最后发现

298
00:10:15.563 --> 00:10:17.219
y其实等于0

299
00:10:17.219 --> 00:10:18.206
这就必须要让假设函数

300
00:10:18.206 --> 00:10:21.940
或者学习算法付出一个很大的代价

301
00:10:21.940 --> 00:10:24.609
反过来

302
00:10:24.610 --> 00:10:25.942
如果h(x)=0

303
00:10:25.950 --> 00:10:27.483
而且y=0

304
00:10:27.483 --> 00:10:28.983
那么假设函数预测对了

305
00:10:29.000 --> 00:10:30.626
预测的是y=0

306
00:10:30.630 --> 00:10:32.371
并且y就是等于0

307
00:10:32.371 --> 00:10:34.376
并且Y就是等于0

308
00:10:34.376 --> 00:10:36.701
那么代价值函数在这点上

309
00:10:36.750 --> 00:10:40.139
应该等于0

310
00:10:40.160 --> 00:10:42.163
在这个视频中

311
00:10:42.163 --> 00:10:43.886
我们定义了

312
00:10:43.886 --> 00:10:46.428
单训练样本的代价函数

313
00:10:46.428 --> 00:10:50.251
凸性分析的内容是超出这门课的范围的

314
00:10:50.270 --> 00:10:51.594
但是可以证明

315
00:10:51.620 --> 00:10:53.080
我们所选的

316
00:10:53.150 --> 00:10:54.774
代价值函数

317
00:10:54.774 --> 00:10:57.926
会给我们一个

318
00:10:57.960 --> 00:11:00.081
凸优化问题

319
00:11:00.081 --> 00:11:01.463
代价函数J(θ)会是一个凸函数

320
00:11:01.463 --> 00:11:04.368
并且没有局部最优值

321
00:11:04.370 --> 00:11:05.691
在下一个视频中

322
00:11:05.691 --> 00:11:07.753
我们会把单训练样本的

323
00:11:07.753 --> 00:11:08.923
代价函数的这些理念

324
00:11:08.923 --> 00:11:10.839
进一步发展

325
00:11:10.839 --> 00:11:12.522
然后给出

326
00:11:12.522 --> 00:11:13.773
整个训练集的代价函数的定义

327
00:11:13.780 --> 00:11:16.104
我们还会找到一种

328
00:11:16.104 --> 00:11:17.404
比我们目前用的

329
00:11:17.404 --> 00:11:19.699
更简单的写法

330
00:11:19.699 --> 00:11:21.016
基于这些推导出的结果

331
00:11:21.030 --> 00:11:22.779
我们将应用梯度下降法

332
00:11:22.779 --> 00:11:25.835
得到我们的逻辑回归算法 【教育无边界字幕组】翻译: 竹二个 校对/审核: 所罗门捷列夫