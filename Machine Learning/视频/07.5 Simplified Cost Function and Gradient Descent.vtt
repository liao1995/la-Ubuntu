WEBVTT

1
00:00:00.310 --> 00:00:02.286
在这段视频中 我们将会找出

2
00:00:02.300 --> 00:00:03.903
一种稍微简单一点的方法来

3
00:00:03.910 --> 00:00:06.513
写代价函数 来替换我们现在用的方法

4
00:00:06.520 --> 00:00:08.252
同时我们还要弄清楚

5
00:00:08.252 --> 00:00:10.779
如何运用梯度下降法

6
00:00:10.779 --> 00:00:13.321
来拟合出逻辑回归的参数

7
00:00:13.321 --> 00:00:14.210
因此 听了这节课

8
00:00:14.210 --> 00:00:15.589
你就应该知道如何

9
00:00:15.589 --> 00:00:19.201
实现一个完整的逻辑回归算法

10
00:00:19.201 --> 00:00:24.802
这就是逻辑回归的代价函数

11
00:00:24.802 --> 00:00:27.613
我们的整体代价函数

12
00:00:27.613 --> 00:00:29.477
不同的训练样本

13
00:00:29.477 --> 00:00:31.003
假设函数 h(x) 对实际值 y(i) 进行预测

14
00:00:31.003 --> 00:00:32.797
所得到的不同误差

15
00:00:32.797 --> 00:00:34.580
算出的 Cost 函数值

16
00:00:34.580 --> 00:00:36.408
并且这是我们之前

17
00:00:36.408 --> 00:00:39.492
算出来的一个单个样本的代价值

18
00:00:39.492 --> 00:00:40.604
我只是想提醒你一下

19
00:00:40.604 --> 00:00:43.514
对于分类问题

20
00:00:43.514 --> 00:00:45.778
我们的训练集

21
00:00:45.778 --> 00:00:47.076
甚至其他不在训练集中的样本

22
00:00:47.076 --> 00:00:48.915
y 的值总是等于0或1的

23
00:00:48.915 --> 00:00:51.056
y 的值总是等于0或1的

24
00:00:51.056 --> 00:00:51.056
对吗？

25
00:00:51.056 --> 00:00:52.150
这就是

26
00:00:52.150 --> 00:00:55.700
y 的数学定义决定的

27
00:00:55.720 --> 00:00:57.430
由于 y 是0或1

28
00:00:57.430 --> 00:00:59.453
我们就可以

29
00:00:59.460 --> 00:01:00.735
想出一个简单的

30
00:01:00.760 --> 00:01:03.001
方式来写这个代价函数

31
00:01:03.001 --> 00:01:04.980
具体来说

32
00:01:04.980 --> 00:01:06.394
为了避免把代价函数

33
00:01:06.410 --> 00:01:07.966
写成两行

34
00:01:07.966 --> 00:01:09.519
避免分成 y=1 或 y=0 两种情况来写

35
00:01:09.519 --> 00:01:11.123
我们要用一种方法

36
00:01:11.130 --> 00:01:12.687
来把这两个式子

37
00:01:12.687 --> 00:01:16.241
合并成一个

38
00:01:16.241 --> 00:01:17.743
这将使我们更方便地

39
00:01:17.743 --> 00:01:19.250
写出代价函数

40
00:01:19.250 --> 00:01:21.493
并推导出梯度下降

41
00:01:21.493 --> 00:01:24.492
具体而言 我们可以如下写出代价函数

42
00:01:24.492 --> 00:01:27.304
Cost(h(x), y) 可以写成

43
00:01:27.304 --> 00:01:29.269
以下的形式

44
00:01:29.269 --> 00:01:31.750
-y log(h(x))- (1-y) log(1-h(x))

45
00:01:31.770 --> 00:01:34.201
-y log(h(x))- (1-y) log(1-h(x))

46
00:01:34.201 --> 00:01:37.730
-y log(h(x))- (1-y) log(1-h(x))

47
00:01:38.060 --> 00:01:41.615
-y log(h(x))- (1-y) log(1-h(x))

48
00:01:41.660 --> 00:01:44.655
-y log(h(x))- (1-y) log(1-h(x))

49
00:01:44.670 --> 00:01:45.824
我马上就会给你演示

50
00:01:45.824 --> 00:01:48.062
这个表达式或

51
00:01:48.062 --> 00:01:51.038
等式与我们已经得出的

52
00:01:51.038 --> 00:01:52.354
代价函数的表达

53
00:01:52.354 --> 00:01:54.195
是完全等效的

54
00:01:54.195 --> 00:01:56.353
并且更加紧凑

55
00:01:56.353 --> 00:02:00.243
让我们来看看为什么会是这样

56
00:02:03.730 --> 00:02:06.190
我们知道有两种可能情况

57
00:02:06.190 --> 00:02:07.210
y 必须是0或1

58
00:02:07.230 --> 00:02:10.857
因此 我们假设 y 等于1

59
00:02:10.857 --> 00:02:12.480
如果 y 是等于

60
00:02:12.480 --> 00:02:14.822
那么这个等式

61
00:02:14.822 --> 00:02:17.603
这个 Cost 值

62
00:02:18.573 --> 00:02:20.172
是等于

63
00:02:20.172 --> 00:02:23.895
如果 y 等于1 那么这一项等于1

64
00:02:23.900 --> 00:02:26.631
1-y 将会等于零 对吧？

65
00:02:26.631 --> 00:02:27.852
如果 y 等于1

66
00:02:27.860 --> 00:02:29.348
那么 1-y

67
00:02:29.370 --> 00:02:32.336
就是1-1 也就是0

68
00:02:32.336 --> 00:02:34.076
因此第二项乘以0

69
00:02:34.076 --> 00:02:36.047
就被消去了

70
00:02:36.047 --> 00:02:37.380
我们只留下了

71
00:02:37.420 --> 00:02:38.631
第一项 y倍的 log 项

72
00:02:38.650 --> 00:02:40.654
-y 乘以

73
00:02:40.654 --> 00:02:42.174
log(h(x)) y等于1

74
00:02:42.174 --> 00:02:43.621
因此就等于

75
00:02:43.630 --> 00:02:46.313
-log(h(x))

76
00:02:46.320 --> 00:02:48.300
这个等式

77
00:02:48.300 --> 00:02:50.050
正是我们在这里的

78
00:02:50.060 --> 00:02:53.276
y=1 的情况

79
00:02:53.276 --> 00:02:55.566
另一种情况是

80
00:02:55.566 --> 00:02:57.275
 如果 y=0

81
00:02:57.290 --> 00:02:58.718
如果是这样的话

82
00:02:58.718 --> 00:03:01.430
那么写出的

83
00:03:01.500 --> 00:03:03.584
Cost 函数就是这样的

84
00:03:03.600 --> 00:03:05.500
如果 y 是等于0

85
00:03:05.500 --> 00:03:08.381
那么这一项就为0

86
00:03:08.381 --> 00:03:10.111
而1-y 在y=0时

87
00:03:10.111 --> 00:03:11.270
1-y 就是0

88
00:03:11.280 --> 00:03:12.528
因为1-y就是

89
00:03:12.530 --> 00:03:14.556
1-0 所以

90
00:03:14.556 --> 00:03:16.650
最后就等于1

91
00:03:16.650 --> 00:03:18.643
这样 Cost 函数

92
00:03:18.643 --> 00:03:22.583
就简化为只有这最后一项

93
00:03:22.583 --> 00:03:22.583
对吧？

94
00:03:22.583 --> 00:03:24.724
因为第一项

95
00:03:24.724 --> 00:03:27.493
在这里乘以零 所以它被消去了

96
00:03:27.493 --> 00:03:28.802
所以 我们只剩下最后的

97
00:03:28.802 --> 00:03:30.486
这一项 也就是

98
00:03:30.510 --> 00:03:32.566
-log(1-h(x))

99
00:03:32.590 --> 00:03:34.243
你可以证明

100
00:03:34.260 --> 00:03:36.013
这里的这一项

101
00:03:36.013 --> 00:03:40.434
就是当y=0时的这一项

102
00:03:40.450 --> 00:03:42.260
因此这表明

103
00:03:42.260 --> 00:03:43.628
这样定义的 Cost 函数

104
00:03:43.628 --> 00:03:45.423
只是把这两个式子

105
00:03:45.423 --> 00:03:47.376
写成一种更紧凑的形式

106
00:03:47.376 --> 00:03:48.757
不需要分 y=1

107
00:03:48.757 --> 00:03:50.284
或 y=0 来写

108
00:03:50.284 --> 00:03:52.014
直接写在一起

109
00:03:52.030 --> 00:03:54.580
只用一行来表示

110
00:03:54.600 --> 00:03:56.449
这样我们就可以写出

111
00:03:56.449 --> 00:03:59.898
逻辑回归的代价函数如下

112
00:03:59.898 --> 00:04:00.628
它是这样的

113
00:04:00.628 --> 00:04:01.746
就是 1/m 乘以后面这个 Cost 函数

114
00:04:01.746 --> 00:04:03.856
在这里放入之前

115
00:04:03.856 --> 00:04:05.123
定义好的 Cost 函数

116
00:04:05.123 --> 00:04:07.255
这个函数就完成了

117
00:04:07.255 --> 00:04:09.767
我们把负号放在外面

118
00:04:09.767 --> 00:04:12.214
我们为什么要把代价函数写成这种形式

119
00:04:12.230 --> 00:04:16.250
似乎我们也可以选择别的方法来写代价函数

120
00:04:16.250 --> 00:04:17.427
在这节课中我没有时间

121
00:04:17.430 --> 00:04:19.171
来介绍有关这个问题的细节

122
00:04:19.171 --> 00:04:21.345
但我可以告诉你

123
00:04:21.345 --> 00:04:23.566
这个式子是从统计学中的

124
00:04:23.566 --> 00:04:25.416
极大似然法得来的

125
00:04:25.440 --> 00:04:26.816
估计 统计学的思路是

126
00:04:26.820 --> 00:04:28.754
如何为不同的模型

127
00:04:28.770 --> 00:04:33.014
有效地找出不同的参数

128
00:04:33.014 --> 00:04:35.843
同时它还有一个很好的性质 它是凸的

129
00:04:35.860 --> 00:04:37.666
因此 这就是基本上

130
00:04:37.666 --> 00:04:40.003
大部分人使用的

131
00:04:40.040 --> 00:04:42.736
逻辑回归代价函数

132
00:04:42.740 --> 00:04:44.264
如果我们不理解这些项

133
00:04:44.264 --> 00:04:45.731
如果你不知道

134
00:04:45.731 --> 00:04:47.280
什么是极大似然估计

135
00:04:47.280 --> 00:04:49.706
不用担心

136
00:04:49.706 --> 00:04:51.240
这里只是一个更深入

137
00:04:51.250 --> 00:04:53.780
更合理的证明而已

138
00:04:53.790 --> 00:04:55.617
在这节课中

139
00:04:55.630 --> 00:04:58.203
我没有时间去仔细讲解

140
00:04:58.203 --> 00:05:00.683
根据这个代价函数

141
00:05:00.683 --> 00:05:02.601
为了拟合出参数

142
00:05:02.601 --> 00:05:04.541
我们怎么办呢？

143
00:05:04.541 --> 00:05:07.896
我们要试图找尽量让 J(θ) 取得最小值的参数 θ

144
00:05:07.910 --> 00:05:10.716
所以我们想要尽量减小这一项

145
00:05:10.716 --> 00:05:15.006
这将我们将得到某个参数 θ

146
00:05:15.006 --> 00:05:17.157
最后 如​​果我们给出一个新的样本

147
00:05:17.157 --> 00:05:18.549
假如某个特征 x

148
00:05:18.549 --> 00:05:20.164
假如某个特征 x

149
00:05:20.164 --> 00:05:21.640
我们可以用拟合训练样本的参数 θ

150
00:05:21.640 --> 00:05:23.980
来输出对假设的预测

151
00:05:23.980 --> 00:05:25.793
来输出对假设的预测

152
00:05:25.800 --> 00:05:27.336
另外提醒你一下

153
00:05:27.336 --> 00:05:28.842
我们假设的输出

154
00:05:28.850 --> 00:05:30.253
实际上就是这个概率值

155
00:05:30.253 --> 00:05:33.001
p(y=1|x;θ)

156
00:05:33.001 --> 00:05:34.656
就是关于 x 以 θ 为参数

157
00:05:34.670 --> 00:05:36.900
 y=1 的概率

158
00:05:36.900 --> 00:05:38.070
你就把这个想成

159
00:05:38.070 --> 00:05:40.613
我们的假设就是

160
00:05:40.613 --> 00:05:43.873
估计 y=1 的概率

161
00:05:43.880 --> 00:05:45.579
所以 接下来要做的事情

162
00:05:45.590 --> 00:05:47.143
就是弄清楚

163
00:05:47.150 --> 00:05:49.520
如何最大限度地

164
00:05:49.520 --> 00:05:51.005
最小化代价函数 J(θ)

165
00:05:51.010 --> 00:05:52.519
作为一个关于 θ 的函数

166
00:05:52.519 --> 00:05:55.625
这样我们才能为训练集拟合出参数 θ

167
00:05:56.390 --> 00:05:57.819
最小化代价函数的方法

168
00:05:57.819 --> 00:06:00.599
是使用梯度下降法(gradient descent)

169
00:06:00.600 --> 00:06:02.225
这是我们的代价函数

170
00:06:02.250 --> 00:06:05.307
如果我们要最小化这个关于 θ 的函数值

171
00:06:05.340 --> 00:06:08.070
这就是我们通常用的梯度下降法的模板

172
00:06:08.070 --> 00:06:09.880
我们要反复更新每个参数

173
00:06:09.880 --> 00:06:12.398
用这个式子来更新

174
00:06:12.398 --> 00:06:14.099
就是用它自己减去

175
00:06:14.099 --> 00:06:17.684
学习率 α 乘以后面的微分项

176
00:06:17.684 --> 00:06:19.219
如果你知道一些微积分的知识

177
00:06:19.219 --> 00:06:20.739
你可以自己动手

178
00:06:20.739 --> 00:06:22.788
算一算这个微分项

179
00:06:22.788 --> 00:06:24.592
看看你算出来的

180
00:06:24.592 --> 00:06:26.664
跟我得到的是不是一样

181
00:06:26.664 --> 00:06:30.538
即使你不知道微积分 也不用担心

182
00:06:30.538 --> 00:06:32.355
如果你计算一下的话

183
00:06:32.370 --> 00:06:34.811
你会得到的是这个式子

184
00:06:34.811 --> 00:06:37.634
我把它写在这里

185
00:06:37.634 --> 00:06:39.047
将后面这个式子

186
00:06:39.047 --> 00:06:41.386
在 i=1 到 m 上求和

187
00:06:41.386 --> 00:06:43.722
其实就是预测误差

188
00:06:43.722 --> 00:06:46.378
乘以 x(i)j

189
00:06:46.390 --> 00:06:48.504
所以你把这个偏导数项

190
00:06:48.504 --> 00:06:49.716
放回到原来式子这里

191
00:06:49.716 --> 00:06:51.210
我们就可以将

192
00:06:51.230 --> 00:06:55.203
梯度下降算法写作如下形式

193
00:06:55.203 --> 00:06:56.393
我做的就是把

194
00:06:56.393 --> 00:06:57.633
前一张幻灯片中的那一行

195
00:06:57.633 --> 00:07:00.163
放到这里了

196
00:07:00.170 --> 00:07:01.454
所以 如果你有 n 个特征

197
00:07:01.454 --> 00:07:03.856
也就是说

198
00:07:03.856 --> 00:07:06.865
参数向量θ 包括

199
00:07:06.865 --> 00:07:08.417
θ0 θ1 θ2

200
00:07:08.417 --> 00:07:10.031
 一直到 θn 

201
00:07:10.031 --> 00:07:11.324
那么你就需要

202
00:07:11.340 --> 00:07:13.930
用这个式子

203
00:07:13.930 --> 00:07:15.920
来同时更新所有 θ 的值

204
00:07:15.950 --> 00:07:17.378
现在 如果你把这个

205
00:07:17.378 --> 00:07:19.498
更新规则和我们之前

206
00:07:19.498 --> 00:07:21.175
用在线性回归上的

207
00:07:21.180 --> 00:07:23.364
进行比较的话

208
00:07:23.370 --> 00:07:25.679
你会惊讶地发现

209
00:07:25.710 --> 00:07:28.958
这个式子正是

210
00:07:28.970 --> 00:07:30.529
我们用来做线性回归梯度下降的

211
00:07:30.550 --> 00:07:31.678
事实上 如果你看一下

212
00:07:31.678 --> 00:07:33.234
前面的视频

213
00:07:33.240 --> 00:07:35.123
再仔细想想这个更新规则

214
00:07:35.123 --> 00:07:36.543
线性梯度下降规则

215
00:07:36.550 --> 00:07:38.418
实际上跟我蓝色框里

216
00:07:38.418 --> 00:07:41.268
写出来的式子是完全一样的

217
00:07:41.268 --> 00:07:43.280
那么 线性回归和

218
00:07:43.280 --> 00:07:45.875
逻辑回归是同一个算法吗？

219
00:07:45.900 --> 00:07:47.415
要回答这个问题

220
00:07:47.415 --> 00:07:49.468
我们要观察逻辑回归

221
00:07:49.500 --> 00:07:51.376
看看发生了哪些变化

222
00:07:51.380 --> 00:07:54.723
实际上 假设的定义发生了变化

223
00:07:54.723 --> 00:07:56.788
所以对于线性回归

224
00:07:56.800 --> 00:07:58.586
假设函数是 h(x) 为

225
00:07:58.620 --> 00:08:01.093
θ 转置乘以 x

226
00:08:01.093 --> 00:08:02.633
而现在逻辑函数假设的定义

227
00:08:02.633 --> 00:08:04.060
已经发生了变化

228
00:08:04.060 --> 00:08:05.460
现在已经变成了

229
00:08:05.460 --> 00:08:07.897
这样的形式

230
00:08:07.910 --> 00:08:09.326
因此 即使更新参数的

231
00:08:09.340 --> 00:08:12.213
规则看起来基本相同

232
00:08:12.230 --> 00:08:13.872
但由于假设的定义

233
00:08:13.872 --> 00:08:15.826
发生了变化 所以逻辑函数的梯度下降

234
00:08:15.826 --> 00:08:19.445
跟线性回归的梯度下降实际上是两个完全不同的东西

235
00:08:19.445 --> 00:08:21.063
在先前的视频中

236
00:08:21.090 --> 00:08:22.889
当我们在谈论线性回归的

237
00:08:22.900 --> 00:08:24.514
梯度下降法时

238
00:08:24.514 --> 00:08:26.128
我们谈到了如何监控

239
00:08:26.160 --> 00:08:29.630
梯度下降法以确保其收敛

240
00:08:29.630 --> 00:08:31.463
我通常也把同样的方法

241
00:08:31.463 --> 00:08:33.354
用在逻辑回归中

242
00:08:33.354 --> 00:08:37.193
来监测梯度下降 以确保它正常收敛

243
00:08:37.220 --> 00:08:38.612
希望你自己能想清楚

244
00:08:38.612 --> 00:08:40.306
如何把同样的方法

245
00:08:40.306 --> 00:08:43.984
应用到逻辑函数的梯度下降中

246
00:08:43.984 --> 00:08:46.603
当使用梯度下降法

247
00:08:46.610 --> 00:08:48.229
来实现逻辑回归时

248
00:08:48.229 --> 00:08:50.404
我们有这些不同的参数 θ

249
00:08:50.404 --> 00:08:52.093
就是 θ0 到 θn

250
00:08:52.130 --> 00:08:55.816
我们需要用这个表达式来更新这些参数

251
00:08:55.816 --> 00:08:58.770
我们还可以使用 for 循环来实现

252
00:08:58.770 --> 00:09:00.926
所以 for i=1 to n

253
00:09:00.926 --> 00:09:03.658
或者 for i=1 to n+1

254
00:09:03.658 --> 00:09:07.217
用一个 for 循环来更新这些参数值

255
00:09:07.217 --> 00:09:08.653
当然 不用 for 循环也是可以的

256
00:09:08.653 --> 00:09:10.588
理想情况下

257
00:09:10.600 --> 00:09:13.163
我们更提倡使用向量化的实现

258
00:09:13.170 --> 00:09:15.072
因此 向量化的实现

259
00:09:15.072 --> 00:09:16.899
可以把所有这些 n 个

260
00:09:16.899 --> 00:09:18.310
参数同时更新

261
00:09:18.310 --> 00:09:21.110
一举搞定

262
00:09:21.110 --> 00:09:22.233
为了检查你自己的理解

263
00:09:22.233 --> 00:09:23.675
是否到位

264
00:09:23.690 --> 00:09:25.223
你可以自己想想

265
00:09:25.223 --> 00:09:27.763
应该怎么样实现这个

266
00:09:27.763 --> 00:09:31.020
向量化的实现方法

267
00:09:31.030 --> 00:09:32.331
好的 现在你知道如何

268
00:09:32.350 --> 00:09:35.079
实现逻辑回归的梯度下降

269
00:09:35.079 --> 00:09:36.706
最后还有一个

270
00:09:36.706 --> 00:09:40.753
我们之前在谈线性回归时讲到的特征缩放

271
00:09:40.753 --> 00:09:42.946
我们看到了特征缩放是如何

272
00:09:42.946 --> 00:09:46.502
提高梯度下降的收敛速度的

273
00:09:46.502 --> 00:09:48.827
这个特征缩放的方法

274
00:09:48.850 --> 00:09:51.712
也适用于逻辑回归

275
00:09:51.730 --> 00:09:54.874
如果你的特征范围差距很大的话

276
00:09:54.890 --> 00:09:56.857
那么应用特征缩放的方法

277
00:09:56.857 --> 00:09:58.941
同样也可以让逻辑回归中

278
00:09:58.941 --> 00:10:01.550
梯度下降收敛更快

279
00:10:01.550 --> 00:10:02.699
就是这样

280
00:10:02.699 --> 00:10:04.552
现在你知道如何实现

281
00:10:04.552 --> 00:10:06.549
逻辑回归

282
00:10:06.549 --> 00:10:08.918
这是一种非常强大

283
00:10:08.918 --> 00:10:10.441
甚至可能世界上使用最广泛的

284
00:10:10.441 --> 00:10:11.982
一种分类算法

285
00:10:11.982 --> 00:10:14.130
而现在你已经知道如何去实现它了 【果壳教育无边界字幕组】翻译：acejunnan 校对：所罗门捷列夫 审核：Naplessss